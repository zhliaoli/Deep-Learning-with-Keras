{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter01 Neural Networks Foundations\n",
    "# 第一章 神经网络基础\n",
    " Artificial neural networks (briefly, nets) represent a class of machine learning models, loosely\n",
    "inspired by studies about the central nervous systems of mammals. Each net is made up of several\n",
    "interconnected neurons, organized in layers, which exchange messages (they fire, in jargon) when\n",
    "certain conditions happen. Initial studies were started in the late 1950s with the introduction of the\n",
    "perceptron (for more information, refer to the article: The Perceptron: A Probabilistic Model for\n",
    "Information Storage and Organization in the Brain, by F. Rosenblatt, Psychological Review, vol.\n",
    "65, pp. 386 - 408, 1958), a two-layer network used for simple operations, and further expanded in the\n",
    "late 1960s with the introduction of the backpropagation algorithm, used for efficient multilayer\n",
    "networks training (according to the articles: Backpropagation through Time: What It Does and How\n",
    "to Do It, by P. J. Werbos, Proceedings of the IEEE, vol. 78, pp. 1550 - 1560, 1990, and A Fast\n",
    "Learning Algorithm for Deep Belief Nets, by G. E. Hinton, S. Osindero, and Y. W. Teh, Neural\n",
    "Computing, vol. 18, pp. 1527 - 1554, 2006). Some studies argue that these techniques have roots\n",
    "dating further back than normally cited (for more information, refer to the article: Deep Learning in\n",
    "Neural Networks: An Overview, by J. Schmidhuber, vol. 61, pp. 85 - 117, 2015). Neural networks\n",
    "were a topic of intensive academic studies until the 1980s, when other simpler approaches became\n",
    "more relevant. However, there has been a resurrection of interest starting from the mid-2000s, thanks\n",
    "to both a breakthrough fast-learning algorithm proposed by G. Hinton (for more information, refer to\n",
    "the articles: The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and\n",
    "Political Forecasting, Neural Networks, by S. Leven, vol. 9, 1996 and Learning Representations by\n",
    "Backpropagating Errors, by D. E. Rumelhart, G. E. Hinton, and R. J. Williams, vol. 323, 1986) and\n",
    "the introduction of GPUs, roughly in 2011, for massive numeric computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These improvements opened the route for modern deep learning, a class of neural networks characterized by a significant number of layers of neurons, which are able to learn rather sophisticated models based on progressive levels of abstraction. People called it deep with 3-5 layers a few years ago, and now it has gone up to 100-200.           \n",
    "\n",
    "这些改进打开了现代深度学习的路线，一种具有大量神经元层的神经网络通过逐层抽象，能够学习相当复杂的模型。 几年前人们调用只能进行3-5层，现在已经达到100-200。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This learning via progressive abstraction resembles vision models that have evolved over millions of years in the human brain. The human visual system is indeed organized into different layers. Our eyes\n",
    "are connected to an area of the brain called the visual cortex V1, which is located in the lower\n",
    "posterior part of our brain. This area is common to many mammals and has the role of discriminating\n",
    "basic properties and small changes in visual orientation, spatial frequencies, and colors. It has been\n",
    "estimated that V1 consists of about 140 million neurons, with 10 billion connections between them.\n",
    "V1 is then connected with other areas V2, V3, V4, V5, and V6, doing progressively more complex\n",
    "image processing and recognition of more sophisticated concepts, such as shapes, faces, animals, and\n",
    "many more. This organization in layers is the result of a huge number of attempts tuned over several\n",
    "100 million years. It has been estimated that there are ~16 billion human cortical neurons, and about\n",
    "10%-25% of the human cortex is devoted to vision (for more information, refer to the article: The\n",
    "Human Brain in Numbers: A Linearly Scaled-up Primate Brain, by S. Herculano-Houzel, vol. 3,\n",
    "2009). Deep learning has taken some inspiration from this layer-based organization of the human\n",
    "visual system: early artificial neuron layers learn basic properties of images, while deeper layers\n",
    "learn more sophisticated concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book covers several major aspects of neural networks by providing working nets coded in\n",
    "Keras, a minimalist and efficient Python library for deep learning computations running on the top of\n",
    "either Google's TensorFlow (for more information, refer to  https://www.tensorflow.org/ ) or University of\n",
    "Montreal's Theano (for more information, refer to  http://deeplearning.net/software/theano/ ) backend. So, let's\n",
    "start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this chapter, we will cover the following topics:      \n",
    "    在本章，我们将介绍一下内容：       \n",
    "* Perceptron\n",
    "* 感知器\n",
    "* ultilayer perceptron\n",
    "* 多层感知器\n",
    "* Activation functions\n",
    "* 激活函数\n",
    "* Gradient descent\n",
    "* 梯度下降\n",
    "* Stochastic gradient descent\n",
    "* 随机梯度下降\n",
    "* Backpropagation\n",
    "* 反向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1Perceptron\n",
    "## 1.1感知器\n",
    "The perceptron is a simple algorithm which, given an input vector $x$ of $n$ values $（x_1，x_2，...，x_n）$ often\n",
    "called input features or simply features, outputs either 1 (yes) or 0 (no). Mathematically, we define a\n",
    "function:      \n",
    "感知器是一个简单的算法，它使得给定的$n$个输入值$（x_1，x_2，...，x_n）$的向量$x$\n",
    "（这些向量通常被称为输入特征或者简单特征），输出为1（是）或0（否）。 在数学上，我们定义一个\n",
    "函数：\n",
    "\n",
    "$$f(x)=\\left\\{\\begin{array}\\\\1\\quad wx+b>0\\\\0\\quad 其他\\end{array}\\right.$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $w$ is a vector of weights, $wx$ is the dot product $ \\sum_{j=1}^n w_j x_j $ , and $b$ is a bias. If you remember\n",
    "elementary geometry, $wx + b$ defines a boundary hyperplane that changes position according to the\n",
    "values assigned to $w$ and $b$. If $x$ lies above the straight line, then the answer is positive, otherwise it is\n",
    "negative. Very simple algorithm! The perception cannot express a maybe answer. It can answer yes\n",
    "(1) or no (0) if we understand how to define $w$ and $b$, that is the training process that will be\n",
    "discussed in the following paragraphs.          \n",
    "这里，$w$是权重向量，$wx$是点乘积$ \\sum_{j=1}^n w_j x_j $，$b$是偏差。 如果你记得\n",
    "基础的几何，应该知道$wx + b$定义了一个边界超平面，我们可以通过设置$w$和$b$的值来改变它的位置\n",
    "。 如果$x$位于直线之上，则结果为正，否则为负。 非常简单的算法！ 感知器不能表达一个\"可能性\"的答案。如果我们了解如何定义$w$和$b$，通过训练， 它可以回答是（1）或否（0）,\n",
    "在以下段落中我们将讨论这一点如何实现。\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first example of Keras code\n",
    "###  第一个Keras代码例子 \n",
    "\n",
    "\n",
    "The initial building block of Keras is a model, and the simplest model is called sequential. A\n",
    "sequential Keras model is a linear pipeline (a stack) of neural networks layers. This code fragment\n",
    "defines a single layer with  12 artificial neurons, and it expects  8 input variables (also known as\n",
    "features):       \n",
    "Keras的初始构建模块是一个模型，最简单的模型称为“序贯模型”。 一个\n",
    "Keras序贯模型是神经网络层的线性管道（堆栈）。 以下代码段\n",
    "定义了一个具有12个人工神经元的单层神经网络，它有8个输入变量（也称为特征）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Activation，需要添加\n",
    "from keras.layers.core import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron can be initialized with specific weights. Keras provides a few choices, the most common of which are listed as follows:                    \n",
    "每个神经元可以用特定的权重进行初始化。 keras提供了几个选择，最常见的选择如下：             \n",
    "\n",
    "* random_uniform : Weights are initialized to uniformly random small values in (-0.05, 0.05). In other words, any value within the given interval is equally likely to be drawn.\n",
    "* random_uniform ：初始化权重值为（-0.05,0.05）中的均匀随机小值。话句话说，给定间隔内的任何值同样可能被生成。             \n",
    "\n",
    "* random_normal : Weights are initialized according to a Gaussian, with a zero mean and small standard deviation of 0.05. For those of you who are not familiar with a Gaussian, think about a symmetric bell curve shape.\n",
    "* random_normal：根据高斯分布初始化权重值，平均值为零，标准偏差为0.05。 若你不熟悉高斯分布，想想一下对称钟形曲线形状。           \n",
    "\n",
    "* zero : All weights are initialized to zero.\n",
    "* zero:所有权重值初始化为零。\n",
    "\n",
    "A full list is available at https://keras.io/initializations/              \n",
    "完整列表可在以下链接查看：https://keras.io/initializations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2Multilayer perceptron — the first example of a network\n",
    "## 1.2多层感知器——第一个网络的例子 \n",
    "\n",
    "In this chapter, we define the first example of a network with multiple linear layers. Historically,\n",
    "perceptron was the name given to a model having one single linear layer, and as a consequence, if it\n",
    "has multiple layers, you would call it multilayer perceptron (MLP). The following image represents\n",
    "a generic neural network with one input layer, one intermediate layer and one output layer.                    \n",
    "在本章中，我们定义了具有多个线性层网络的第一个例子。按惯例，\n",
    "感知器是具有一个单一线性层的模型的名称，因此，如果它聚\n",
    "有多层，你可以称之为多层感知器（MLP）。 以下图像代表\n",
    "具有一个输入层，一个中间层和一个输出层的通用神经网络。\n",
    "![](http://i2.muimg.com/588926/fba5e9e8ccf3b51d.png)\n",
    "In the preceding diagram, each node in the first layer receives an input and fires according to the\n",
    "predefined local decision boundaries. Then the output of the first layer is passed to the second layer,\n",
    "the results of which are passed to the final output layer consisting of one single neuron. It is\n",
    "interesting to note that this layered organization vaguely resembles the patterns of human vision we\n",
    "discussed earlier.           \n",
    "在上图中，第一层中的每个节点接收到一个输入，并根据此触发\n",
    "预定义的局部决策界限。 然后将第一层的输出传递到第二层，\n",
    "其结果被传递到由一个单个神经元组成的最终输出层。 \n",
    "有趣的是，这种分层组织有些类似于前面讨论过的人类视觉模式。\n",
    "\n",
    "\n",
    "The net is dense, meaning that each neuron in a layer is connected to all neurons\n",
    "located in the previous layer and to all the neurons in the following layer.     \n",
    "由此所组成的网络是密集的，这意味着一层中的每个神经元都连接到位于上一层的所有神经元\n",
    "和下一层的所有神经元。\n",
    "\n",
    "### Problems in training the perceptron and a solution\n",
    "### 训练感知器和解决方案的问题\n",
    "Let's consider a single neuron; what are the best choices for the weight $w$ and the bias $b$? Ideally, we\n",
    "would like to provide a set of training examples and let the computer adjust the weight and the bias in\n",
    "such a way that the errors produced in the output are minimized. In order to make this a bit more\n",
    "concrete, let's suppose we have a set of images of cats and another separate set of images not\n",
    "containing cats. For the sake of simplicity, assume that each neuron looks at a single input pixel value.\n",
    "While the computer processes these images, we would like our neuron to adjust its weights and bias\n",
    "so that we have fewer and fewer images wrongly recognized as non-cats. This approach seems very\n",
    "intuitive, but it requires that a small change in weights (and/or bias) causes only a small change in\n",
    "outputs.                     \n",
    "我们来考虑单个神经元; 权重$w$和偏差$b$的最佳选择是什么？ 理想情况下，我们\n",
    "想提供一套训练集，让计算机调整权重值和偏差值来使输出中产生的误差最小化。 为了使这一点更具体，我们假设我们有一组包含猫的图像和另一组单独不包含猫的图像。为了简单起见，假设每个神经元都看到单个输入像素值。\n",
    "当计算机处理这些图像时，我们希望我们的神经元调整其重量和偏差，使得我们有越来越少的图像被错误地识别为非猫。 这种方法似乎非常直观，但是它要求权重（和/或偏差）的微小变化只会在输出上产生微小变化。\n",
    "\n",
    "\n",
    "If we have a big output jump, we cannot progressively learn (rather than trying things in all possible directions—a process known as exhaustive search—without knowing if we are improving). After all,kids learn little by little. Unfortunately, the perceptron does not show this little-by-little behavior. A perceptron is either 0 or 1 and that is a big jump and it will not help it to learn, as shown in the following graph:   \n",
    "如果我们有一个巨大的输出增量，我们不能逐步学习（并非在所有可能的方向上尝试——这一的详尽搜索过程我们不知道是否在改进）。 毕竟，孩子们是一点一点学习的。 不幸的是，感知器并不表现出这种一点一点学习的行为。 感知器是0或1，这是一个大的增量，它不会帮助它学习，如下图所示：      \n",
    "![](http://i1.piimg.com/588926/e8b393495ed9048f.png)\n",
    "\n",
    "We need something different, smoother. We need a function that progressively changes from 0 to 1 with no discontinuity. Mathematically, this means that we need a continuous function that allows us to compute the derivative.  \n",
    "我们需要不同的东西，它应该具有更平滑的性质。 我们需要一个从0到1逐渐变化的函数，没有不连续性。 在数学上，这意味着我们需要一个连续的函数，使我们能够计算导数。     \n",
    "\n",
    "### Activation function — sigmoid\n",
    "### 激活函数——sigmoid\n",
    "The sigmoid function is defined as follows:      \n",
    "sigmoid函数的定义如下： \n",
    "$$\\sigma(x)= \\frac{1}{1+e^{-x}} $$\n",
    "As represented in the following graph, it has small output changes in (0, 1) when the input varies in. Mathematically, the function is continuous. A typical sigmoid function is represented in the following graph:        \n",
    "如下图所示，当输入变化时，（0,1）中的输出变化小。数学上，该函数是连续的。 典型的igmoid函数如下图所示：    \n",
    "![](http://i2.muimg.com/588926/1461c537b1135098.png)          \n",
    "A neuron can use the sigmoid for computing the nonlinear function $\\sigma(z=wx+b)$ . Note that, if $z=wx+b$ is very large and positive, then $e^{-z}\\rightarrow0$ , so $\\sigma(z)\\rightarrow1$  , while if $z=wx+b$ is very large and negative  ，$e^{-z}\\rightarrow\\infty$ ,so  $e^{z}\\rightarrow 0$  . In other words, a neuron with sigmoid activation has a behavior similar to the perceptron, but the changes are gradual and output values, such as 0.5539 or 0.123191, are perfectly legitimate. In this sense, a sigmoid neuron can answer maybe.              \n",
    "神经元可以使用sigmoid 来计算非线性函数$\\sigma(z=wx+b)$。 请注意，如果$z=wx+b$ 是非常大和正的，那么 $e^{-z}\\rightarrow0$，所以$\\sigma(z)\\rightarrow1$，而如果$z=wx+b$ 非常大和负的，$e^{-z}\\rightarrow\\infty$，所以$e^{z}\\rightarrow 0$。 换句话说，具有sigmoid激活的神经元具有类似于感知器的行为，但变化是渐进的，输出值（如0.5539或0.123191）是完全合适的。 在这个意义上，Sigmoid神经元可以被解决之前提出的问题。       \n",
    "\n",
    "### Activation function — ReLU\n",
    "### 激活函数——ReLU\n",
    "The sigmoid is not the only kind of smooth activation function used for neural networks. Recently, a very simple function called rectified linear unit (ReLU) became very popular because it generates very good experimental results. A ReLU is simply defined as $f(x)=max(0,x)$ , and the nonlinear function is represented in the following graph. As you can see in the following graph, the function is zero for negative values, and it grows linearly for positive values:\n",
    "sigmoid不是唯一一种用于神经网络的平滑激活函数。 最近，一个非常简单的称为线性纠正单元（ReLU）的函数变得非常受欢迎，因为它产生了非常好的实验结果。 ReLU被简单地定义为$f(x)=max(0,x)$，并且非线性函数如下图所示。对于负值，函数为零，对于正值，它将线性增长： \n",
    "![](http://i1.piimg.com/588926/2046f27aee8eec0a.png)\n",
    "\n",
    "### Activation functions\n",
    "### 激活函数    \n",
    "Sigmoid and ReLU are generally called activation functions in neural network jargon. In the Testing different optimizers in Keras section, we will see that those gradual changes, typical of sigmoid and ReLU functions, are the basic building blocks to developing a learning algorithm which adapts little by little, by progressively reducing the mistakes made by our nets. An example of using the activation function $\\sigma$ with the $(x_1 , x_2 , ..., x_m )$ input vector, $(w_1 , w_2 , ..., w_m )$ weight vector, $b$ bias, and $\\sum$summation is given in the following diagram:     \n",
    "Sigmoid和ReLU通常称为神经网络术语中的激活函数。 在Keras中测试不同的优化器时，我们将看到，Sigmoid和ReLU函数的典型变化是开发学习算法的基本构件，逐渐减少了网络的错误。 下图给出了一个例子，其中 $(x_1 , x_2 , ..., x_m )$为输入向量，$(w_1 , w_2 , ..., w_m )$为使用激活函数$\\sigma$ 的权重向量，$b$为偏量,$\\sum$表示总和： \n",
    "![](http://i4.buimg.com/588926/c36f694da7481427.png)    \n",
    "Keras supports a number of activation functions, and a full list is available at  https://keras.io/activations/ .             \n",
    "keras提供了多种激活函数，完整列表如下： https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3A real example — recognizing handwritten digits\n",
    "## 1.3一个真正的例子——识别手写数字\n",
    "In this section, we will build a network that can recognize handwritten numbers. For achieving this goal, we use MNIST(for more information, refer to  http://yann.lecun.com/exdb/mnist/ ) , a database of handwritten digits made up of a training set of 60,000 examples and a test set of 10,000 examples.The training examples are annotated by humans with the correct answer. For instance, if the handwritten digit is the number three, then three is simply the label associated with that example.     \n",
    "在本节中，我们将构建一个可识别手写数字的网络。 为了实现这一目标，我们使用MNIST（更多信息参见 http://yann.lecun.com/exdb/mnist/ ） ， 一个由60,000个样本的训练集和10,000个样本的测试集组成的手写数字数据库。训练样本由人类给出正确的答案。 例如，如果手写数字是“三”，那么“三”就是与该样本相关联的标签。         \n",
    "\n",
    "In machine learning, when a dataset with correct answers is available, we say that we can perform a form of supervised learning. In this case, we can use training examples for tuning up our net. Testing examples also have the correct answer associated with each digit. In this case, however, the idea is to pretend that the label is unknown, let the network do the prediction, and then later on, reconsider the label to evaluate how well our neural network has learned to recognize digits. So, not unsurprisingly,testing examples are just used to test our net.          \n",
    "在机器学习中，当具有正确答案的数据集存在时，我们则可以进行有监督学习。 在这种情况下，我们可以使用训练样本来调整网络。 测试样本也具有与每个数字相关联的正确答案。 然而，在这种情况下，这个想法是假装标签是未知的，让网络进行预测，然后再重新考虑标签来评估我们的神经网络学到识别数字的程度。 所以，毫不奇怪，测试样本只是用来测试我们的网络。        \n",
    "\n",
    "Each MNIST image is in gray scale, and it consists of 28 x 28 pixels. A subset of these numbers is represented in the following diagram:      \n",
    "每个MNIST图像都是灰度的，它由28 x 28个像素组成。 这些数字的一个子集如下图所示：         \n",
    "![](http://i1.piimg.com/588926/ad61e1983f936e8f.png)\n",
    "\n",
    "### One-hot encoding \n",
    "### One-hot 编码         \n",
    "In many applications, it is convenient to transform categorical (non-numerical) features into numerical variables. For instance, the categorical feature digit with the value d in [0-9] can be encoded into a binary vector with 10 positions, which always has 0 value, except the d-th position where a 1 is present. This type of representation is called one-hot encoding (OHE) and is very common in data mining when the learning algorithm is specialized for dealing with numerical functions.    \n",
    "在许多应用中，将分类（非数字）特征转换为数值变量是很方便的。 例如，具有[0-9]中值d的分类特征数字可以被编码为具有10个位置的二进制向量，除了第d个位置为1，其他位置始终为0值。 这种类型的表示法被称为 One-hot编码（OHE），当学习算法专门用于处理数字函数时，在数据挖掘中非常普遍。\n",
    "\n",
    "### Defining a simple neural net in Keras\n",
    "### 在Keras中定义简单的神经网络\n",
    "Here, we use Keras to define a network that recognizes MNIST handwritten digits. We start with a very simple neural network and then progressively improve it.          \n",
    "在这里，我们使用Keras来定义一个识别MNIST手写数字的网络。 我们从一个非常简单的神经网络开始，然后逐步改进。      \n",
    "\n",
    "Keras provides suitable libraries to load the dataset and split it into training sets  X_train , used for fine-tuning our net, and tests set  X_test , used for assessing the performance. Data is converted into float32 for supporting GPU computation and normalized to [0, 1]. In addition, we load the true labels into  Y_train and  Y_test respectively and perform a one-hot encoding on them. Let's see the code:       \n",
    "Keras提供了合适的库来加载数据集，并将其分成训练集X_train，用于微调我们的网络，测试集X_test用于评估性能。 数据转换为float32，用于支持GPU计算并归一化为[0,1]。 另外，我们分别将真正的标签加载到Y_train和Y_test中，并对它们执行一次one-hot编码。 我们来看看代码：     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility 每次生成的随机数相同\n",
    "# network and training 网络和训练\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits 输出位数=数字位数\n",
    "OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter  SGD优化器，在本章后面解释\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION 取全样本的多少比例作训练集\n",
    "# data: shuffled and split between train and test sets\n",
    "#在训练和测试集中打乱并切分\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "#X_train是60000行28x28值 - >在60000 x 784重新整形\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize 归一化\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里可以输出检查下X_train, y_train，X_test或 y_test\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The input layer has a neuron associated with each pixel in the image for a total of 28 x 28 = 784 neurons, one for each pixel in the MNIST images.       \n",
    "输入层具有与图像中的每个像素相关联的神经元，总共28×28 = 784个神经元，一个用于MNIST图像中的每个像素。     \n",
    "\n",
    "\n",
    "Typically, the values associated with each pixel are normalized in the range [0, 1] (which means that the intensity of each pixel is divided by 255, the maximum intensity value). The output is 10 classes,one for each digit.        \n",
    "通常，与每个像素相关联的值在范围[0,1]中被归一化（这意味着每个像素的灰度除以255，最大灰度值）。 输出为10个类，每个数字一个。 \n",
    "\n",
    "The final layer is a single neuron with activation function softmax, which is a generalization of the sigmoid function. Softmax squashes a k-dimensional vector of arbitrary real values into a k-dimensional vector of real values in the range (0, 1). In our case, it aggregates 10 answers provided by the previous layer with 10 neurons:       \n",
    "最后一层是具有激活函数softmax的单个神经元，其是sigmoid函数的泛化。 Softmax将任意实数值的k维向量压缩为范围（0,1）中实值的k维向量。 在我们的例子中，它聚合了上一层提供的10个神经元的10个答案："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850.0\n",
      "Trainable params: 7,850.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs  10个输出\n",
    "# final stage is softmax  最后阶段是softmax \n",
    "#搭建网络\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we define the model, we have to compile it so that it can be executed by the Keras backend (either Theano or TensorFlow). There are a few choices to be made during compilation:     \n",
    "一旦我们定义了模型，我们必须编译它，以便它可以由Keras后端（Theano或TensorFlow）执行。 在编译期间有几个选择： \n",
    "\n",
    "\n",
    "* We need to select the optimizer that is the specific algorithm used to update weights while we train our model\n",
    "* 我们需要选择优化器，这是在训练我们的模型时用于更新权重的特定算法\n",
    "\n",
    "* We need to select the objective function that is used by the optimizer to navigate the space of weights (frequently,objective functions are called loss function, and the process of optimization is defined as a process of loss minimization)   \n",
    "* 我们需要选择优化器使用的目标函数来导航权重空间（通常，目标函数称为损失函数，优化过程被定义为损失最小化的过程）\n",
    "\n",
    "* We need to evaluate the trained model\n",
    "* 我们需要评估被训练的模型\n",
    "\n",
    "Some common choices for the objective function (a complete list of Keras objective functions is at  https://keras.io/objectives/ ) are as follows:        \n",
    "目标函数的一些常见选择（Keras目标函数的完整列表在https://keras.io/objectives/ ）如下：\n",
    "\n",
    "* MSE: This is the mean squared error between the predictions and the true values.Mathematically, if  $\\Upsilon$  is a vector of $n$ predictions, and $Y$ is the vector of $n$ observed values, then they satisfy the following equation:\n",
    "* MSE：这是预测值和真实值之间的均方误差。从数学上讲，如果$\\Upsilon$是$n$个预测的向量，$Y$是$n$个观测值的向量，则满足以下等式：\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^n {(\\Upsilon-Y)}^2  $$       \n",
    "\n",
    "These objective functions average all the mistakes made for each prediction, and if the prediction is far from the true value, then this distance is made more evident by the squaring operation.        \n",
    "这些目标函数平均了每个预测的所有错误，并且如果预测远离真实值，则通过平方运算使该距离更加明显。      \n",
    "\n",
    "* Binary cross-entropy: This is the binary logarithmic loss. Suppose that our model predicts p while the target is t, then the binary cross-entropy is defined as follows:\n",
    "* Binary cross-entropy：这是二进制对数损失。 假设我们的模型在目标为t时预测p，则二进制交叉熵定义如下：\n",
    "$$-tlog(p)-(1-t)log(1-p)$$\n",
    "This objective function is suitable for binary labels prediction.         \n",
    "该目标函数适用于二进制标签预测。\n",
    "* Categorical cross-entropy: This is the multiclass logarithmic loss. If the target is $t_{i,j}$ and the prediction is $p _{i,j}$ , then the categorical cross-entropy is this:\n",
    "* Categorical cross-entropy：这是多类对数损失。 如果目标是 $t_{i,j}$ 和预测是$p _{i,j}$，则分类交叉熵是：\n",
    "$$L_i=-\\sum_jt_{i,j}log(p_{i,j})$$\n",
    "\n",
    "    This objective function is suitable for multiclass labels predictions. It is also the default choice in association with softmax activation.          \n",
    "    该目标函数适用于多类标签预测。 它也是与softmax激活相关联的默认选择。\n",
    " \n",
    "Some common choices for metrics (a complete list of Keras metrics is at  https://keras.io/metrics/ ) are as\n",
    "follows:    \n",
    "指标的一些常见选择（Keras指标的完整列表位于https://keras.io/metrics/ ）与之相同如下：\n",
    "* Accuracy: This is the proportion of correct predictions with respect to the targets    \n",
    "* Precision: This denotes how many selected items are relevant for a multilabel classification     \n",
    "* Recall: This denotes how many selected items are relevant for a multilabel classification\n",
    "* Accuracy:这是对目标的正确预测的比例\n",
    "* Precision:这表示多个选择的项目与多标签分类相关\n",
    "* Recall:这表示多个选择的项目与多标签分类相关\n",
    "\n",
    "Metrics are similar to objective functions, with the only difference that they are not used for training a model but only for evaluating a model. Compiling a model in Keras is easy:        \n",
    "度量与目标函数相似，唯一的区别是它们不用于训练模型，而只用于评估模型。 在Keras编译一个模型很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is compiled, it can be then trained with the  fit() function, which specifies a few parameters:       \n",
    "一旦模型被编译，就可以用fit（）函数进行训练，该函数指定了一些参数： \n",
    "* epochs : This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized. \n",
    "* epochs :这是模型暴露于训练集的次数。 在每次迭代时，优化器尝试调整权重，使目标函数最小化。                              \n",
    "\n",
    "* batch_size : This is the number of training instances observed before the optimizer performs a weight update. \n",
    "* batch_size：这是在优化器执行权重更新之前观察到的训练实例的样本数量。                                           \n",
    "\n",
    "Training a model in Keras is very simple. Suppose we want to iterate for  NB_EPOCH steps:    \n",
    "在Keras中训练一个模型很简单。 假设我们要迭代NB_EPOCH步："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s - loss: 1.3633 - acc: 0.6796 - val_loss: 0.8904 - val_acc: 0.8246\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.7913 - acc: 0.8272 - val_loss: 0.6572 - val_acc: 0.8546\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.6436 - acc: 0.8497 - val_loss: 0.5625 - val_acc: 0.8681\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.5717 - acc: 0.8602 - val_loss: 0.5098 - val_acc: 0.8765\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.5276 - acc: 0.8678 - val_loss: 0.4758 - val_acc: 0.8826\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4973 - acc: 0.8726 - val_loss: 0.4515 - val_acc: 0.8866\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4748 - acc: 0.8775 - val_loss: 0.4333 - val_acc: 0.8882\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4574 - acc: 0.8803 - val_loss: 0.4189 - val_acc: 0.8920\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4433 - acc: 0.8834 - val_loss: 0.4075 - val_acc: 0.8939\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4317 - acc: 0.8850 - val_loss: 0.3977 - val_acc: 0.8966\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4218 - acc: 0.8873 - val_loss: 0.3896 - val_acc: 0.8984\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4134 - acc: 0.8888 - val_loss: 0.3827 - val_acc: 0.8995\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4060 - acc: 0.8902 - val_loss: 0.3766 - val_acc: 0.9003\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3995 - acc: 0.8918 - val_loss: 0.3712 - val_acc: 0.9013\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3936 - acc: 0.8928 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3884 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.9031\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3837 - acc: 0.8950 - val_loss: 0.3582 - val_acc: 0.9033\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3794 - acc: 0.8962 - val_loss: 0.3546 - val_acc: 0.9039\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3755 - acc: 0.8970 - val_loss: 0.3514 - val_acc: 0.9048\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3718 - acc: 0.8979 - val_loss: 0.3485 - val_acc: 0.9053\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3685 - acc: 0.8985 - val_loss: 0.3457 - val_acc: 0.9058\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3653 - acc: 0.8995 - val_loss: 0.3431 - val_acc: 0.9058\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3625 - acc: 0.8999 - val_loss: 0.3407 - val_acc: 0.9063\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3598 - acc: 0.9008 - val_loss: 0.3385 - val_acc: 0.9070\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3572 - acc: 0.9012 - val_loss: 0.3364 - val_acc: 0.9074\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3345 - val_acc: 0.9084\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3525 - acc: 0.9022 - val_loss: 0.3326 - val_acc: 0.9082\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3504 - acc: 0.9032 - val_loss: 0.3311 - val_acc: 0.9090\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3484 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.9094\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3465 - acc: 0.9041 - val_loss: 0.3277 - val_acc: 0.9097\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3447 - acc: 0.9044 - val_loss: 0.3264 - val_acc: 0.9097\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3430 - acc: 0.9047 - val_loss: 0.3249 - val_acc: 0.9097\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3413 - acc: 0.9051 - val_loss: 0.3235 - val_acc: 0.9103\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3397 - acc: 0.9056 - val_loss: 0.3222 - val_acc: 0.9104\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3382 - acc: 0.9058 - val_loss: 0.3211 - val_acc: 0.9110\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3368 - acc: 0.9062 - val_loss: 0.3198 - val_acc: 0.9110\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3353 - acc: 0.9069 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3340 - acc: 0.9075 - val_loss: 0.3177 - val_acc: 0.9120\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3327 - acc: 0.9075 - val_loss: 0.3166 - val_acc: 0.9122\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3314 - acc: 0.9078 - val_loss: 0.3159 - val_acc: 0.9118\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3303 - acc: 0.9080 - val_loss: 0.3147 - val_acc: 0.9127\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3291 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9132\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3280 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.9132\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3270 - acc: 0.9091 - val_loss: 0.3121 - val_acc: 0.9132\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3259 - acc: 0.9093 - val_loss: 0.3113 - val_acc: 0.9135\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3105 - val_acc: 0.9137\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3239 - acc: 0.9105 - val_loss: 0.3098 - val_acc: 0.9141\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3230 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3221 - acc: 0.9102 - val_loss: 0.3083 - val_acc: 0.9151\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3212 - acc: 0.9109 - val_loss: 0.3075 - val_acc: 0.9150\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3204 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.9150\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3195 - acc: 0.9112 - val_loss: 0.3063 - val_acc: 0.9148\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3187 - acc: 0.9114 - val_loss: 0.3057 - val_acc: 0.9153\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3180 - acc: 0.9117 - val_loss: 0.3050 - val_acc: 0.9148\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3171 - acc: 0.9121 - val_loss: 0.3044 - val_acc: 0.9149\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3164 - acc: 0.9121 - val_loss: 0.3037 - val_acc: 0.9156\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3157 - acc: 0.9128 - val_loss: 0.3034 - val_acc: 0.9152\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3149 - acc: 0.9121 - val_loss: 0.3029 - val_acc: 0.9148\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3143 - acc: 0.9128 - val_loss: 0.3022 - val_acc: 0.9151\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3136 - acc: 0.9129 - val_loss: 0.3016 - val_acc: 0.9161\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3130 - acc: 0.9133 - val_loss: 0.3011 - val_acc: 0.9158\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3123 - acc: 0.9131 - val_loss: 0.3007 - val_acc: 0.9151\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3117 - acc: 0.9136 - val_loss: 0.3003 - val_acc: 0.9156\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3110 - acc: 0.9137 - val_loss: 0.2997 - val_acc: 0.9158\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3105 - acc: 0.9137 - val_loss: 0.2992 - val_acc: 0.9159\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3098 - acc: 0.9138 - val_loss: 0.2988 - val_acc: 0.9161\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3093 - acc: 0.9141 - val_loss: 0.2983 - val_acc: 0.9165\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3087 - acc: 0.9139 - val_loss: 0.2979 - val_acc: 0.9166\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3082 - acc: 0.9144 - val_loss: 0.2976 - val_acc: 0.9164\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3077 - acc: 0.9145 - val_loss: 0.2971 - val_acc: 0.9166\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3071 - acc: 0.9146 - val_loss: 0.2967 - val_acc: 0.9172\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3066 - acc: 0.9147 - val_loss: 0.2964 - val_acc: 0.9167\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3061 - acc: 0.9151 - val_loss: 0.2960 - val_acc: 0.9169\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3056 - acc: 0.9150 - val_loss: 0.2956 - val_acc: 0.9173\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2952 - val_acc: 0.9177\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3046 - acc: 0.9152 - val_loss: 0.2950 - val_acc: 0.9173\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3042 - acc: 0.9154 - val_loss: 0.2945 - val_acc: 0.9172\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3037 - acc: 0.9154 - val_loss: 0.2942 - val_acc: 0.9176\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3032 - acc: 0.9157 - val_loss: 0.2939 - val_acc: 0.9179\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3028 - acc: 0.9156 - val_loss: 0.2936 - val_acc: 0.9177\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3024 - acc: 0.9157 - val_loss: 0.2933 - val_acc: 0.9179\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3019 - acc: 0.9157 - val_loss: 0.2930 - val_acc: 0.9178\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3015 - acc: 0.9160 - val_loss: 0.2926 - val_acc: 0.9182\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3011 - acc: 0.9161 - val_loss: 0.2924 - val_acc: 0.9179\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3007 - acc: 0.9165 - val_loss: 0.2920 - val_acc: 0.9184\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3003 - acc: 0.9164 - val_loss: 0.2918 - val_acc: 0.9185\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2999 - acc: 0.9165 - val_loss: 0.2914 - val_acc: 0.9185\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2995 - acc: 0.9166 - val_loss: 0.2911 - val_acc: 0.9188\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2991 - acc: 0.9167 - val_loss: 0.2909 - val_acc: 0.9191\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2988 - acc: 0.9169 - val_loss: 0.2906 - val_acc: 0.9191\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2984 - acc: 0.9168 - val_loss: 0.2903 - val_acc: 0.9192\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2981 - acc: 0.9170 - val_loss: 0.2901 - val_acc: 0.9196\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2977 - acc: 0.9171 - val_loss: 0.2898 - val_acc: 0.9195\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2973 - acc: 0.9172 - val_loss: 0.2895 - val_acc: 0.9196\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2970 - acc: 0.9174 - val_loss: 0.2894 - val_acc: 0.9196\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2967 - acc: 0.9174 - val_loss: 0.2891 - val_acc: 0.9198\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2963 - acc: 0.9176 - val_loss: 0.2889 - val_acc: 0.9197\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2960 - acc: 0.9174 - val_loss: 0.2886 - val_acc: 0.9202\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2957 - acc: 0.9176 - val_loss: 0.2884 - val_acc: 0.9202\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2953 - acc: 0.9178 - val_loss: 0.2882 - val_acc: 0.9200\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2950 - acc: 0.9179 - val_loss: 0.2879 - val_acc: 0.9201\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2947 - acc: 0.9180 - val_loss: 0.2877 - val_acc: 0.9204\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2944 - acc: 0.9180 - val_loss: 0.2875 - val_acc: 0.9202\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2941 - acc: 0.9184 - val_loss: 0.2873 - val_acc: 0.9202\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2938 - acc: 0.9183 - val_loss: 0.2871 - val_acc: 0.9206\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2935 - acc: 0.9183 - val_loss: 0.2868 - val_acc: 0.9202\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2932 - acc: 0.9186 - val_loss: 0.2867 - val_acc: 0.9206\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2929 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9208\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2927 - acc: 0.9185 - val_loss: 0.2863 - val_acc: 0.9206\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2923 - acc: 0.9187 - val_loss: 0.2860 - val_acc: 0.9204\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2921 - acc: 0.9184 - val_loss: 0.2858 - val_acc: 0.9210\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2918 - acc: 0.9187 - val_loss: 0.2857 - val_acc: 0.9207\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2915 - acc: 0.9189 - val_loss: 0.2854 - val_acc: 0.9210\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2913 - acc: 0.9188 - val_loss: 0.2853 - val_acc: 0.9211\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2910 - acc: 0.9189 - val_loss: 0.2852 - val_acc: 0.9205\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2908 - acc: 0.9189 - val_loss: 0.2849 - val_acc: 0.9213\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2905 - acc: 0.9193 - val_loss: 0.2847 - val_acc: 0.9213\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2902 - acc: 0.9192 - val_loss: 0.2846 - val_acc: 0.9212\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2900 - acc: 0.9191 - val_loss: 0.2844 - val_acc: 0.9212\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2842 - val_acc: 0.9212\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2895 - acc: 0.9191 - val_loss: 0.2841 - val_acc: 0.9212\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2892 - acc: 0.9192 - val_loss: 0.2840 - val_acc: 0.9212\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2890 - acc: 0.9194 - val_loss: 0.2838 - val_acc: 0.9211\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2888 - acc: 0.9197 - val_loss: 0.2837 - val_acc: 0.9210\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2885 - acc: 0.9193 - val_loss: 0.2835 - val_acc: 0.9207\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2883 - acc: 0.9197 - val_loss: 0.2834 - val_acc: 0.9217\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2881 - acc: 0.9194 - val_loss: 0.2832 - val_acc: 0.9212\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2879 - acc: 0.9194 - val_loss: 0.2830 - val_acc: 0.9210\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2876 - acc: 0.9196 - val_loss: 0.2828 - val_acc: 0.9217\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2874 - acc: 0.9197 - val_loss: 0.2826 - val_acc: 0.9216\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2871 - acc: 0.9200 - val_loss: 0.2827 - val_acc: 0.9211\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2870 - acc: 0.9197 - val_loss: 0.2824 - val_acc: 0.9213\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2868 - acc: 0.9198 - val_loss: 0.2823 - val_acc: 0.9216\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2866 - acc: 0.9199 - val_loss: 0.2822 - val_acc: 0.9214\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2863 - acc: 0.9203 - val_loss: 0.2820 - val_acc: 0.9213\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2861 - acc: 0.9196 - val_loss: 0.2818 - val_acc: 0.9215\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2859 - acc: 0.9198 - val_loss: 0.2818 - val_acc: 0.9217\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2857 - acc: 0.9203 - val_loss: 0.2815 - val_acc: 0.9218\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2855 - acc: 0.9203 - val_loss: 0.2814 - val_acc: 0.9215\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2853 - acc: 0.9201 - val_loss: 0.2812 - val_acc: 0.9216\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2852 - acc: 0.9204 - val_loss: 0.2811 - val_acc: 0.9217\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2849 - acc: 0.9201 - val_loss: 0.2810 - val_acc: 0.9217\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2848 - acc: 0.9205 - val_loss: 0.2809 - val_acc: 0.9219\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2846 - acc: 0.9208 - val_loss: 0.2808 - val_acc: 0.9217\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2844 - acc: 0.9207 - val_loss: 0.2806 - val_acc: 0.9221\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2841 - acc: 0.9206 - val_loss: 0.2806 - val_acc: 0.9220\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2840 - acc: 0.9207 - val_loss: 0.2804 - val_acc: 0.9217\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2838 - acc: 0.9209 - val_loss: 0.2803 - val_acc: 0.9218\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2836 - acc: 0.9208 - val_loss: 0.2802 - val_acc: 0.9216\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2800 - val_acc: 0.9225\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2833 - acc: 0.9210 - val_loss: 0.2799 - val_acc: 0.9226\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2831 - acc: 0.9211 - val_loss: 0.2798 - val_acc: 0.9222\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2829 - acc: 0.9207 - val_loss: 0.2797 - val_acc: 0.9224\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2827 - acc: 0.9209 - val_loss: 0.2796 - val_acc: 0.9222\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2795 - val_acc: 0.9225\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2824 - acc: 0.9210 - val_loss: 0.2794 - val_acc: 0.9224\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2822 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9224\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2821 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9226\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2819 - acc: 0.9214 - val_loss: 0.2791 - val_acc: 0.9226\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2817 - acc: 0.9213 - val_loss: 0.2790 - val_acc: 0.9225\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2816 - acc: 0.9214 - val_loss: 0.2789 - val_acc: 0.9222\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2788 - val_acc: 0.9227\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2812 - acc: 0.9213 - val_loss: 0.2787 - val_acc: 0.9225\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2811 - acc: 0.9216 - val_loss: 0.2786 - val_acc: 0.9225\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2785 - val_acc: 0.9227\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2807 - acc: 0.9216 - val_loss: 0.2784 - val_acc: 0.9225\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2806 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9227\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2804 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9228\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2803 - acc: 0.9216 - val_loss: 0.2782 - val_acc: 0.9227\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2781 - val_acc: 0.9227\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2800 - acc: 0.9220 - val_loss: 0.2780 - val_acc: 0.9226\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2798 - acc: 0.9218 - val_loss: 0.2778 - val_acc: 0.9231\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2797 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.9229\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2796 - acc: 0.9217 - val_loss: 0.2777 - val_acc: 0.9227\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2794 - acc: 0.9218 - val_loss: 0.2776 - val_acc: 0.9232\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2793 - acc: 0.9220 - val_loss: 0.2775 - val_acc: 0.9232\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2791 - acc: 0.9219 - val_loss: 0.2774 - val_acc: 0.9234\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2790 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9228\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2788 - acc: 0.9221 - val_loss: 0.2773 - val_acc: 0.9232\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2787 - acc: 0.9221 - val_loss: 0.2771 - val_acc: 0.9235\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2770 - val_acc: 0.9232\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2784 - acc: 0.9220 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2783 - acc: 0.9223 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2781 - acc: 0.9223 - val_loss: 0.2768 - val_acc: 0.9230\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9233\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2779 - acc: 0.9223 - val_loss: 0.2766 - val_acc: 0.9236\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2777 - acc: 0.9224 - val_loss: 0.2766 - val_acc: 0.9233\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2776 - acc: 0.9226 - val_loss: 0.2765 - val_acc: 0.9236\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2775 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2773 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2772 - acc: 0.9225 - val_loss: 0.2763 - val_acc: 0.9237\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2762 - val_acc: 0.9238\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9237\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2768 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9236\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2767 - acc: 0.9231 - val_loss: 0.2760 - val_acc: 0.9239\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2766 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9241\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2765 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9242\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2763 - acc: 0.9231 - val_loss: 0.2758 - val_acc: 0.9236\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2762 - acc: 0.9229 - val_loss: 0.2757 - val_acc: 0.9241\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2761 - acc: 0.9230 - val_loss: 0.2756 - val_acc: 0.9241\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserved part of the training set for validation. The key idea is that we reserve a part of the training data for measuring the performance on the validation while training. This is a good practice to follow for any machine learning task, which we will adopt in all our examples.       \n",
    "我们保留了一部分验证的训练集。 关键的概念是我们保留一部分训练数据，用于测量训练时的验证性能。 我们在所有任何机器学习任务的例子中，这是采用的良好做法。   \n",
    "\n",
    "Once the model is trained, we can evaluate it on the test set that contains new unseen examples. In this way, we can get the minimal value reached by the objective function and best value reached by the evaluation metric.       \n",
    "一旦模型被训练，我们可以在包含新的未知样本的测试集上进行评估。 这样，我们可以得到目标函数达到的最小值和评估指标达到的最佳值。\n",
    "\n",
    "Note that the training set and the test set are, of course, rigorously separated. There is no point in evaluating a model on an example that has already been used for training. Learning is essentially a process intended to generalize unseen observations and not to memorize what is already known:    \n",
    "请注意，训练集和测试集当然是严格分开的。 在已经用于培训的例子上评估一个模型是没有意义的。 学习本质上是一个旨在概括不可见观察的过程，而不是记住已知的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8992/10000 [=========================>....] - ETA: 0sTest score: 0.27738585037\n",
      "Test accuracy: 0.9227\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, congratulations, you have just defined your first neural network in Keras. A few lines of code, and your computer is able to recognize handwritten numbers. Let's run the code and see what the performance is.      \n",
    "所以，恭喜，你刚刚在Keras中定义了你的第一个神经网络。 几行代码，您的计算机能够识别手写号码。 我们来运行代码，看看性能是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGDCAYAAAD9K8D/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33f16b1908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘出分类准确率曲线\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running a simple Keras net and establishing a baseline\n",
    "### 运行简单的Keras网络并建立一条参考线\n",
    "So let's see what will happen when we run the code in the following screenshot:     \n",
    "所以我们来看看当我们在下面的截图中运行代码时会发生什么：     \n",
    "![](http://i4.buimg.com/588926/d9aadf0896fc9238.png)       \n",
    "First, the net architecture is dumped, and we can see the different types of layers used, their output shape, how many parameters they need to optimize, and how they are connected. Then, the network is trained on 48,000 samples, and 12,000 are reserved for validation. Once the neural model is built, it is then tested on 10,000 samples. As you can see, Keras is internally using TensorFlow as a backend system for computation. For now, we don't go into the internals on how the training happens, but we can notice that the program runs for 200 iterations, and each time, the accuracy improves. When the training ends, we test our model on the test set and achieve about 92.30 % accuracy on training,92.41% on validation, and 92.27% on the test.           \n",
    "首先，网络架构被转储，我们可以看到使用的不同类型的层，它们的输出形状，需要优化的参数以及它们的连接方式。 然后，该网络接受了48,000个样本的训练，12,000个被保留以用于验证。 一旦构建了神经模型，然后就可在10,000个样本上进行测试。 如您所见，Keras在内部使用TensorFlow作为后端系统进行计算。 现在，我们不关心训练在内部如何产生，但是我们可以注意到该程序运行了200次迭代，每次迭代精度都会提高。 训练结束后，我们对测试模型进行测试，训练准确率达92.30％，验证率达到92.41％，测试率达92.27％。      \n",
    "\n",
    "This means that a bit less than one handwritten character out of ten is not correctly recognized. We can certainly do better than that. In the following screenshot, we can see the test accuracy:       \n",
    "这意味着十分之一以下的一个手写字符不能被正确识别。 我们当然可以做得比那更好。 在下面的截图中，我们可以看到测试的准确性：     \n",
    "![](http://i4.buimg.com/588926/cdcb9660a3181d15.png)\n",
    "![](http://i4.buimg.com/588926/cebdb88934fc266f.png)   \n",
    "\n",
    "### Improving the simple net in Keras with hidden layers\n",
    "### 利用隐藏层改进Keras中的简单网络\n",
    "We have a baseline accuracy of 92.30% on training, 92.41% on validation, and 92.27% on the test.This is a good starting point, but we can certainly improve it. Let's see how.     \n",
    "我们有一个训练准确度为92.30％的参考线，验证率为92.41％，测试时为92.27％。这是一个很好的起点，但我们当然可以改善。 我们来看看如何实现。     \n",
    "\n",
    "A first improvement is to add additional layers to our network. So, after the input layer, we have a first dense layer with the  N_HIDDEN neurons and an activation function  relu . This additional layer is considered hidden because it is not directly connected to either the input or the output. After the first hidden layer, we have a second hidden layer, again with the  N_HIDDEN neurons, followed by an output layer with 10 neurons, each of which will fire when the relative digit is recognized. The following code defines this new network:        \n",
    "第一个改进方法是为我们的网络添加额外的层。 因此，在输入层之后，我们有一个具有N_HIDDEN个神经元和激活函数relu的第一dense层。 此附加层被认为是隐藏的，因为它不直接连接到输入或输出。 在第一个隐藏层之后，我们有一个第二个隐藏层，同样是N_HIDDEN个神经元，后面是一个10个神经元的输出层，当相关数字被识别时，它们都会触发。 以下代码定义了这个新网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282.0\n",
      "Trainable params: 118,282.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s - loss: 1.4827 - acc: 0.6231 - val_loss: 0.7582 - val_acc: 0.8287\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.6048 - acc: 0.8463 - val_loss: 0.4549 - val_acc: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.4397 - acc: 0.8802 - val_loss: 0.3709 - val_acc: 0.9020\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.3767 - acc: 0.8952 - val_loss: 0.3321 - val_acc: 0.9081\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.3415 - acc: 0.9026 - val_loss: 0.3055 - val_acc: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.3175 - acc: 0.9086 - val_loss: 0.2880 - val_acc: 0.9183\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2989 - acc: 0.9136 - val_loss: 0.2727 - val_acc: 0.9222\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2839 - acc: 0.9180 - val_loss: 0.2608 - val_acc: 0.9267\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2714 - acc: 0.9216 - val_loss: 0.2504 - val_acc: 0.9297\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2602 - acc: 0.9252 - val_loss: 0.2430 - val_acc: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2502 - acc: 0.9285 - val_loss: 0.2341 - val_acc: 0.9333\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2409 - acc: 0.9301 - val_loss: 0.2271 - val_acc: 0.9353\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2325 - acc: 0.9334 - val_loss: 0.2228 - val_acc: 0.9366\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2253 - acc: 0.9354 - val_loss: 0.2147 - val_acc: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2182 - acc: 0.9375 - val_loss: 0.2082 - val_acc: 0.9412\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2116 - acc: 0.9393 - val_loss: 0.2030 - val_acc: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2055 - acc: 0.9413 - val_loss: 0.1981 - val_acc: 0.9446\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.1996 - acc: 0.9429 - val_loss: 0.1932 - val_acc: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.1942 - acc: 0.9432 - val_loss: 0.1894 - val_acc: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.1891 - acc: 0.9455 - val_loss: 0.1850 - val_acc: 0.9497\n",
      " 9888/10000 [============================>.] - ETA: 0sTest score: 0.186031601541\n",
      "Test accuracy: 0.9462\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the code and see which result we get with this multilayer network. Not bad. By adding two hidden layers, we reached 94.55% on the training set, 94.97% on validation, and 94.62% on the test.This means that we gained an additional 2.2% accuracy on the test with respect to the previous network. However, we dramatically reduced the number of iterations from 200 to 20. That's good, but we want more.       \n",
    "我们来运行代码，看看这个多层网络得到的结果。还不错， 通过增加两个隐藏层次，我们达到的准确率在训练集中为94.55％，验证时为94.97％，测试时为94.62％。这意味着我们比以前的网络测试提高了2.33％的准确性。 然而，我们将迭代次数从200个减少到了20个。这很好，但是我们想要更多。       \n",
    "\n",
    "\n",
    "If you want, you can play by yourself and see what happens if you add only one hidden layer instead of two, or if you add more than two layers. I leave this experiment as an exercise. The following screenshot shows the output of the preceding example:          \n",
    "如果你愿意，你可以自己玩，看看如果你只添加一个隐藏层而不是两个，或者如果你添加两个以上的层会发生什么。 我把这个实验作为一个练习。 以下屏幕截图显示了上述示例的输出：     \n",
    "![](http://i2.muimg.com/588926/69a8fecb2d2b09ad.png)    \n",
    "\n",
    "### Further improving the simple net in Keras with dropout     \n",
    "### 利用dropout层进一步改进Keras中的简单网络        \n",
    "Now our baseline is 94.55% on the training set, 94.96% on validation, and 94.60% on the test. A second improvement is very simple. We decide to randomly drop with the dropout probability some of the values propagated inside our internal dense network of hidden layers. In machine learning, this is a well-known form of regularization. Surprisingly enough, this idea of randomly dropping a few values can improve our performance:          \n",
    "现在我们在训练集中准确率参考值为94.55％，验证时为94.96％，测试时为94.60％。 第二个改进很简单。 我们决定按概率随机删除一些包含有隐藏层的全连接(dense)网络的内部中传递的值（参考理解keras文档：Dropout将在训练过程中每次更新参数时随机断开一定百分比的输入神经元连接，Dropout层用于防止过拟合。）。 在机器学习中，这是一种众所周知的正规化形式。 令人惊讶的是，这种随机删除几个值的想法可以提高我们的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282.0\n",
      "Trainable params: 118,282.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 3s - loss: 1.7402 - acc: 0.4540 - val_loss: 0.9290 - val_acc: 0.8123\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.9230 - acc: 0.7230 - val_loss: 0.5400 - val_acc: 0.8653\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.6935 - acc: 0.7880 - val_loss: 0.4297 - val_acc: 0.8885\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.5947 - acc: 0.8209 - val_loss: 0.3790 - val_acc: 0.8977\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.5347 - acc: 0.8394 - val_loss: 0.3455 - val_acc: 0.9040\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.4976 - acc: 0.8523 - val_loss: 0.3231 - val_acc: 0.9107\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.4616 - acc: 0.8629 - val_loss: 0.3048 - val_acc: 0.9129\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.4386 - acc: 0.8687 - val_loss: 0.2895 - val_acc: 0.9174\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.4181 - acc: 0.8762 - val_loss: 0.2776 - val_acc: 0.9199\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3990 - acc: 0.8837 - val_loss: 0.2656 - val_acc: 0.9233\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3819 - acc: 0.8877 - val_loss: 0.2551 - val_acc: 0.9257\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3688 - acc: 0.8920 - val_loss: 0.2465 - val_acc: 0.9281\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3571 - acc: 0.8943 - val_loss: 0.2388 - val_acc: 0.9300\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.3466 - acc: 0.8992 - val_loss: 0.2319 - val_acc: 0.9322\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.3358 - acc: 0.9016 - val_loss: 0.2261 - val_acc: 0.9338\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3244 - acc: 0.9055 - val_loss: 0.2180 - val_acc: 0.9352\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3142 - acc: 0.9086 - val_loss: 0.2122 - val_acc: 0.9375\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3102 - acc: 0.9095 - val_loss: 0.2075 - val_acc: 0.9391\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.3018 - acc: 0.9117 - val_loss: 0.2018 - val_acc: 0.9409\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2931 - acc: 0.9132 - val_loss: 0.1973 - val_acc: 0.9420\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2866 - acc: 0.9172 - val_loss: 0.1920 - val_acc: 0.9437\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2789 - acc: 0.9170 - val_loss: 0.1878 - val_acc: 0.9447\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.2730 - acc: 0.9200 - val_loss: 0.1841 - val_acc: 0.9464\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.2686 - acc: 0.9211 - val_loss: 0.1810 - val_acc: 0.9466\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2618 - acc: 0.9235 - val_loss: 0.1770 - val_acc: 0.9478\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2584 - acc: 0.9250 - val_loss: 0.1736 - val_acc: 0.9487\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2539 - acc: 0.9253 - val_loss: 0.1706 - val_acc: 0.9494\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2453 - acc: 0.9276 - val_loss: 0.1676 - val_acc: 0.9503\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2427 - acc: 0.9275 - val_loss: 0.1640 - val_acc: 0.9518\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2397 - acc: 0.9296 - val_loss: 0.1616 - val_acc: 0.9523\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2361 - acc: 0.9304 - val_loss: 0.1589 - val_acc: 0.9533\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.2320 - acc: 0.9305 - val_loss: 0.1568 - val_acc: 0.9547\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.2284 - acc: 0.9327 - val_loss: 0.1534 - val_acc: 0.9553\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2257 - acc: 0.9325 - val_loss: 0.1519 - val_acc: 0.9550\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2214 - acc: 0.9354 - val_loss: 0.1501 - val_acc: 0.9557\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2169 - acc: 0.9352 - val_loss: 0.1485 - val_acc: 0.9564\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2125 - acc: 0.9376 - val_loss: 0.1459 - val_acc: 0.9570\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2122 - acc: 0.9372 - val_loss: 0.1432 - val_acc: 0.9579\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2091 - acc: 0.9386 - val_loss: 0.1422 - val_acc: 0.9576\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2042 - acc: 0.9391 - val_loss: 0.1410 - val_acc: 0.9583\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2027 - acc: 0.9397 - val_loss: 0.1396 - val_acc: 0.9584\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1985 - acc: 0.9416 - val_loss: 0.1367 - val_acc: 0.9593\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.2003 - acc: 0.9408 - val_loss: 0.1350 - val_acc: 0.9606\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1953 - acc: 0.9423 - val_loss: 0.1337 - val_acc: 0.9607\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1921 - acc: 0.9431 - val_loss: 0.1332 - val_acc: 0.9600\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1901 - acc: 0.9443 - val_loss: 0.1316 - val_acc: 0.9616\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1876 - acc: 0.9450 - val_loss: 0.1299 - val_acc: 0.9612\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1867 - acc: 0.9442 - val_loss: 0.1301 - val_acc: 0.9618\n",
      "Epoch 49/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1864 - acc: 0.9453 - val_loss: 0.1283 - val_acc: 0.9611\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1803 - acc: 0.9462 - val_loss: 0.1267 - val_acc: 0.9623\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1823 - acc: 0.9465 - val_loss: 0.1254 - val_acc: 0.9634\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1794 - acc: 0.9460 - val_loss: 0.1244 - val_acc: 0.9631\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1753 - acc: 0.9480 - val_loss: 0.1233 - val_acc: 0.9633\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1738 - acc: 0.9478 - val_loss: 0.1220 - val_acc: 0.9637\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1736 - acc: 0.9491 - val_loss: 0.1209 - val_acc: 0.9646\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1719 - acc: 0.9486 - val_loss: 0.1208 - val_acc: 0.9638\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1692 - acc: 0.9504 - val_loss: 0.1188 - val_acc: 0.9649\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1664 - acc: 0.9507 - val_loss: 0.1187 - val_acc: 0.9651\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1682 - acc: 0.9500 - val_loss: 0.1172 - val_acc: 0.9654\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1647 - acc: 0.9515 - val_loss: 0.1165 - val_acc: 0.9652\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1615 - acc: 0.9523 - val_loss: 0.1157 - val_acc: 0.9655\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1592 - acc: 0.9526 - val_loss: 0.1149 - val_acc: 0.9657\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1587 - acc: 0.9534 - val_loss: 0.1142 - val_acc: 0.9657\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1564 - acc: 0.9531 - val_loss: 0.1126 - val_acc: 0.9667\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1560 - acc: 0.9539 - val_loss: 0.1128 - val_acc: 0.9670\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1572 - acc: 0.9534 - val_loss: 0.1120 - val_acc: 0.9664\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1554 - acc: 0.9546 - val_loss: 0.1105 - val_acc: 0.9671\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1525 - acc: 0.9544 - val_loss: 0.1102 - val_acc: 0.9672\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1524 - acc: 0.9553 - val_loss: 0.1089 - val_acc: 0.9676\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1502 - acc: 0.9551 - val_loss: 0.1086 - val_acc: 0.9678\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1478 - acc: 0.9567 - val_loss: 0.1082 - val_acc: 0.9680\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1450 - acc: 0.9567 - val_loss: 0.1073 - val_acc: 0.9685\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1462 - acc: 0.9568 - val_loss: 0.1068 - val_acc: 0.9680\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1439 - acc: 0.9583 - val_loss: 0.1067 - val_acc: 0.9682\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1447 - acc: 0.9566 - val_loss: 0.1059 - val_acc: 0.9681\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1414 - acc: 0.9580 - val_loss: 0.1060 - val_acc: 0.9684\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 4s - loss: 0.1421 - acc: 0.9580 - val_loss: 0.1055 - val_acc: 0.9680\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1399 - acc: 0.9589 - val_loss: 0.1044 - val_acc: 0.9689\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1415 - acc: 0.9571 - val_loss: 0.1041 - val_acc: 0.9687\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1393 - acc: 0.9595 - val_loss: 0.1033 - val_acc: 0.9689\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1370 - acc: 0.9592 - val_loss: 0.1035 - val_acc: 0.9688\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1366 - acc: 0.9579 - val_loss: 0.1031 - val_acc: 0.9687\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1344 - acc: 0.9599 - val_loss: 0.1020 - val_acc: 0.9692\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1338 - acc: 0.9601 - val_loss: 0.1014 - val_acc: 0.9695\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1337 - acc: 0.9602 - val_loss: 0.1015 - val_acc: 0.9697\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1346 - acc: 0.9602 - val_loss: 0.1006 - val_acc: 0.9696\n",
      "Epoch 87/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1305 - acc: 0.9609 - val_loss: 0.1004 - val_acc: 0.9703\n",
      "Epoch 88/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1321 - acc: 0.9595 - val_loss: 0.1000 - val_acc: 0.9697\n",
      "Epoch 89/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1304 - acc: 0.9609 - val_loss: 0.0991 - val_acc: 0.9703\n",
      "Epoch 90/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1321 - acc: 0.9602 - val_loss: 0.0987 - val_acc: 0.9705\n",
      "Epoch 91/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1286 - acc: 0.9621 - val_loss: 0.0982 - val_acc: 0.9709\n",
      "Epoch 92/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1318 - acc: 0.9601 - val_loss: 0.0986 - val_acc: 0.9714\n",
      "Epoch 93/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1285 - acc: 0.9616 - val_loss: 0.0977 - val_acc: 0.9711\n",
      "Epoch 94/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1250 - acc: 0.9622 - val_loss: 0.0975 - val_acc: 0.9709\n",
      "Epoch 95/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1265 - acc: 0.9628 - val_loss: 0.0974 - val_acc: 0.9712\n",
      "Epoch 96/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1240 - acc: 0.9625 - val_loss: 0.0969 - val_acc: 0.9716\n",
      "Epoch 97/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1241 - acc: 0.9621 - val_loss: 0.0960 - val_acc: 0.9712\n",
      "Epoch 98/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1235 - acc: 0.9630 - val_loss: 0.0965 - val_acc: 0.9717\n",
      "Epoch 99/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1217 - acc: 0.9644 - val_loss: 0.0957 - val_acc: 0.9717\n",
      "Epoch 100/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1212 - acc: 0.9635 - val_loss: 0.0957 - val_acc: 0.9719\n",
      "Epoch 101/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1227 - acc: 0.9630 - val_loss: 0.0960 - val_acc: 0.9723\n",
      "Epoch 102/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1215 - acc: 0.9641 - val_loss: 0.0947 - val_acc: 0.9721\n",
      "Epoch 103/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1193 - acc: 0.9645 - val_loss: 0.0950 - val_acc: 0.9720\n",
      "Epoch 104/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1177 - acc: 0.9647 - val_loss: 0.0942 - val_acc: 0.9722\n",
      "Epoch 105/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1164 - acc: 0.9655 - val_loss: 0.0943 - val_acc: 0.9727\n",
      "Epoch 106/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1170 - acc: 0.9650 - val_loss: 0.0939 - val_acc: 0.9723\n",
      "Epoch 107/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1169 - acc: 0.9646 - val_loss: 0.0940 - val_acc: 0.9730\n",
      "Epoch 108/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1139 - acc: 0.9665 - val_loss: 0.0933 - val_acc: 0.9724\n",
      "Epoch 109/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1146 - acc: 0.9658 - val_loss: 0.0933 - val_acc: 0.9734\n",
      "Epoch 110/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1140 - acc: 0.9660 - val_loss: 0.0928 - val_acc: 0.9729\n",
      "Epoch 111/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1146 - acc: 0.9657 - val_loss: 0.0927 - val_acc: 0.9726\n",
      "Epoch 112/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1117 - acc: 0.9661 - val_loss: 0.0917 - val_acc: 0.9738\n",
      "Epoch 113/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1126 - acc: 0.9659 - val_loss: 0.0921 - val_acc: 0.9731\n",
      "Epoch 114/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1145 - acc: 0.9656 - val_loss: 0.0915 - val_acc: 0.9737\n",
      "Epoch 115/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1114 - acc: 0.9663 - val_loss: 0.0914 - val_acc: 0.9741\n",
      "Epoch 116/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1087 - acc: 0.9672 - val_loss: 0.0911 - val_acc: 0.9737\n",
      "Epoch 117/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1115 - acc: 0.9664 - val_loss: 0.0912 - val_acc: 0.9737\n",
      "Epoch 118/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1087 - acc: 0.9669 - val_loss: 0.0908 - val_acc: 0.9739\n",
      "Epoch 119/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1117 - acc: 0.9659 - val_loss: 0.0910 - val_acc: 0.9741\n",
      "Epoch 120/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1070 - acc: 0.9675 - val_loss: 0.0901 - val_acc: 0.9743\n",
      "Epoch 121/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1084 - acc: 0.9670 - val_loss: 0.0904 - val_acc: 0.9744\n",
      "Epoch 122/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1074 - acc: 0.9674 - val_loss: 0.0896 - val_acc: 0.9747\n",
      "Epoch 123/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1043 - acc: 0.9678 - val_loss: 0.0891 - val_acc: 0.9747\n",
      "Epoch 124/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1047 - acc: 0.9684 - val_loss: 0.0895 - val_acc: 0.9745\n",
      "Epoch 125/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1043 - acc: 0.9691 - val_loss: 0.0893 - val_acc: 0.9742\n",
      "Epoch 126/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1035 - acc: 0.9686 - val_loss: 0.0888 - val_acc: 0.9745\n",
      "Epoch 127/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1033 - acc: 0.9684 - val_loss: 0.0889 - val_acc: 0.9748\n",
      "Epoch 128/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1042 - acc: 0.9686 - val_loss: 0.0884 - val_acc: 0.9751\n",
      "Epoch 129/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1050 - acc: 0.9676 - val_loss: 0.0883 - val_acc: 0.9753\n",
      "Epoch 130/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1039 - acc: 0.9690 - val_loss: 0.0883 - val_acc: 0.9752\n",
      "Epoch 131/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1025 - acc: 0.9689 - val_loss: 0.0876 - val_acc: 0.9750\n",
      "Epoch 132/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0999 - acc: 0.9702 - val_loss: 0.0879 - val_acc: 0.9750\n",
      "Epoch 133/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1009 - acc: 0.9687 - val_loss: 0.0876 - val_acc: 0.9751\n",
      "Epoch 134/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0989 - acc: 0.9686 - val_loss: 0.0877 - val_acc: 0.9748\n",
      "Epoch 135/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1008 - acc: 0.9693 - val_loss: 0.0880 - val_acc: 0.9748\n",
      "Epoch 136/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.1001 - acc: 0.9704 - val_loss: 0.0876 - val_acc: 0.9753\n",
      "Epoch 137/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0996 - acc: 0.9694 - val_loss: 0.0878 - val_acc: 0.9757\n",
      "Epoch 138/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.1003 - acc: 0.9690 - val_loss: 0.0875 - val_acc: 0.9755\n",
      "Epoch 139/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0975 - acc: 0.9705 - val_loss: 0.0873 - val_acc: 0.9754\n",
      "Epoch 140/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0965 - acc: 0.9708 - val_loss: 0.0869 - val_acc: 0.9757\n",
      "Epoch 141/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0971 - acc: 0.9699 - val_loss: 0.0867 - val_acc: 0.9759\n",
      "Epoch 142/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0952 - acc: 0.9707 - val_loss: 0.0865 - val_acc: 0.9762\n",
      "Epoch 143/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0969 - acc: 0.9702 - val_loss: 0.0868 - val_acc: 0.9757\n",
      "Epoch 144/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0946 - acc: 0.9711 - val_loss: 0.0865 - val_acc: 0.9757\n",
      "Epoch 145/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0961 - acc: 0.9707 - val_loss: 0.0859 - val_acc: 0.9759\n",
      "Epoch 146/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0938 - acc: 0.9721 - val_loss: 0.0863 - val_acc: 0.9756\n",
      "Epoch 147/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0937 - acc: 0.9713 - val_loss: 0.0865 - val_acc: 0.9761\n",
      "Epoch 148/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0949 - acc: 0.9710 - val_loss: 0.0861 - val_acc: 0.9759\n",
      "Epoch 149/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0925 - acc: 0.9721 - val_loss: 0.0856 - val_acc: 0.9757\n",
      "Epoch 150/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0917 - acc: 0.9720 - val_loss: 0.0863 - val_acc: 0.9757\n",
      "Epoch 151/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0942 - acc: 0.9717 - val_loss: 0.0857 - val_acc: 0.9758\n",
      "Epoch 152/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0923 - acc: 0.9724 - val_loss: 0.0853 - val_acc: 0.9762\n",
      "Epoch 153/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0892 - acc: 0.9728 - val_loss: 0.0853 - val_acc: 0.9757\n",
      "Epoch 154/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0916 - acc: 0.9723 - val_loss: 0.0855 - val_acc: 0.9758\n",
      "Epoch 155/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0850 - val_acc: 0.9762\n",
      "Epoch 156/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0910 - acc: 0.9725 - val_loss: 0.0849 - val_acc: 0.9757\n",
      "Epoch 157/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0901 - acc: 0.9731 - val_loss: 0.0850 - val_acc: 0.9759\n",
      "Epoch 158/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0885 - acc: 0.9730 - val_loss: 0.0855 - val_acc: 0.9762\n",
      "Epoch 159/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0878 - acc: 0.9727 - val_loss: 0.0846 - val_acc: 0.9763\n",
      "Epoch 160/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0894 - acc: 0.9730 - val_loss: 0.0848 - val_acc: 0.9762\n",
      "Epoch 161/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0888 - acc: 0.9729 - val_loss: 0.0842 - val_acc: 0.9767\n",
      "Epoch 162/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0885 - acc: 0.9735 - val_loss: 0.0843 - val_acc: 0.9766\n",
      "Epoch 163/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0870 - acc: 0.9733 - val_loss: 0.0846 - val_acc: 0.9763\n",
      "Epoch 164/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0878 - acc: 0.9731 - val_loss: 0.0841 - val_acc: 0.9768\n",
      "Epoch 165/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0860 - acc: 0.9734 - val_loss: 0.0839 - val_acc: 0.9762\n",
      "Epoch 166/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0862 - acc: 0.9731 - val_loss: 0.0848 - val_acc: 0.9762\n",
      "Epoch 167/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0857 - acc: 0.9738 - val_loss: 0.0847 - val_acc: 0.9760\n",
      "Epoch 168/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0837 - acc: 0.9746 - val_loss: 0.0843 - val_acc: 0.9761\n",
      "Epoch 169/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0853 - acc: 0.9741 - val_loss: 0.0839 - val_acc: 0.9757\n",
      "Epoch 170/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0871 - acc: 0.9736 - val_loss: 0.0834 - val_acc: 0.9763\n",
      "Epoch 171/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0854 - acc: 0.9739 - val_loss: 0.0832 - val_acc: 0.9766\n",
      "Epoch 172/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0845 - acc: 0.9738 - val_loss: 0.0833 - val_acc: 0.9762\n",
      "Epoch 173/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0858 - acc: 0.9740 - val_loss: 0.0840 - val_acc: 0.9762\n",
      "Epoch 174/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0826 - acc: 0.9748 - val_loss: 0.0833 - val_acc: 0.9763\n",
      "Epoch 175/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0857 - acc: 0.9734 - val_loss: 0.0836 - val_acc: 0.9763\n",
      "Epoch 176/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0807 - acc: 0.9752 - val_loss: 0.0837 - val_acc: 0.9765\n",
      "Epoch 177/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0823 - acc: 0.9751 - val_loss: 0.0829 - val_acc: 0.9769\n",
      "Epoch 178/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0825 - acc: 0.9748 - val_loss: 0.0828 - val_acc: 0.9767\n",
      "Epoch 179/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0812 - acc: 0.9751 - val_loss: 0.0823 - val_acc: 0.9765\n",
      "Epoch 180/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0834 - acc: 0.9739 - val_loss: 0.0831 - val_acc: 0.9765\n",
      "Epoch 181/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0812 - acc: 0.9748 - val_loss: 0.0819 - val_acc: 0.9770\n",
      "Epoch 182/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0784 - acc: 0.9757 - val_loss: 0.0826 - val_acc: 0.9771\n",
      "Epoch 183/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0803 - acc: 0.9758 - val_loss: 0.0825 - val_acc: 0.9767\n",
      "Epoch 184/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0808 - acc: 0.9752 - val_loss: 0.0818 - val_acc: 0.9766\n",
      "Epoch 185/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0791 - acc: 0.9755 - val_loss: 0.0822 - val_acc: 0.9763\n",
      "Epoch 186/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0806 - acc: 0.9755 - val_loss: 0.0825 - val_acc: 0.9767\n",
      "Epoch 187/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0786 - acc: 0.9756 - val_loss: 0.0821 - val_acc: 0.9763\n",
      "Epoch 188/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0795 - acc: 0.9754 - val_loss: 0.0813 - val_acc: 0.9767\n",
      "Epoch 189/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0775 - acc: 0.9758 - val_loss: 0.0821 - val_acc: 0.9766\n",
      "Epoch 190/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0775 - acc: 0.9759 - val_loss: 0.0822 - val_acc: 0.9764\n",
      "Epoch 191/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0780 - acc: 0.9768 - val_loss: 0.0822 - val_acc: 0.9764\n",
      "Epoch 192/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0797 - acc: 0.9757 - val_loss: 0.0821 - val_acc: 0.9768\n",
      "Epoch 193/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0776 - acc: 0.9761 - val_loss: 0.0818 - val_acc: 0.9766\n",
      "Epoch 194/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0780 - acc: 0.9762 - val_loss: 0.0818 - val_acc: 0.9770\n",
      "Epoch 195/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0768 - acc: 0.9762 - val_loss: 0.0820 - val_acc: 0.9765\n",
      "Epoch 196/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0742 - acc: 0.9772 - val_loss: 0.0817 - val_acc: 0.9764\n",
      "Epoch 197/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0770 - acc: 0.9757 - val_loss: 0.0816 - val_acc: 0.9767\n",
      "Epoch 198/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0749 - acc: 0.9769 - val_loss: 0.0814 - val_acc: 0.9767\n",
      "Epoch 199/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0750 - acc: 0.9770 - val_loss: 0.0811 - val_acc: 0.9768\n",
      "Epoch 200/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0742 - acc: 0.9768 - val_loss: 0.0809 - val_acc: 0.9762\n",
      "Epoch 201/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0746 - acc: 0.9763 - val_loss: 0.0815 - val_acc: 0.9767\n",
      "Epoch 202/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0781 - acc: 0.9762 - val_loss: 0.0811 - val_acc: 0.9775\n",
      "Epoch 203/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0748 - acc: 0.9771 - val_loss: 0.0810 - val_acc: 0.9768\n",
      "Epoch 204/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0719 - acc: 0.9773 - val_loss: 0.0810 - val_acc: 0.9769\n",
      "Epoch 205/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0753 - acc: 0.9769 - val_loss: 0.0813 - val_acc: 0.9770\n",
      "Epoch 206/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0731 - acc: 0.9774 - val_loss: 0.0812 - val_acc: 0.9772\n",
      "Epoch 207/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0724 - acc: 0.9773 - val_loss: 0.0811 - val_acc: 0.9762\n",
      "Epoch 208/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0725 - acc: 0.9771 - val_loss: 0.0813 - val_acc: 0.9772\n",
      "Epoch 209/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0731 - acc: 0.9773 - val_loss: 0.0814 - val_acc: 0.9768\n",
      "Epoch 210/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0711 - acc: 0.9777 - val_loss: 0.0815 - val_acc: 0.9767\n",
      "Epoch 211/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0734 - acc: 0.9766 - val_loss: 0.0819 - val_acc: 0.9769\n",
      "Epoch 212/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0739 - acc: 0.9770 - val_loss: 0.0814 - val_acc: 0.9767\n",
      "Epoch 213/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0724 - acc: 0.9778 - val_loss: 0.0809 - val_acc: 0.9776\n",
      "Epoch 214/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0722 - acc: 0.9779 - val_loss: 0.0810 - val_acc: 0.9776\n",
      "Epoch 215/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0705 - acc: 0.9786 - val_loss: 0.0809 - val_acc: 0.9770\n",
      "Epoch 216/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0707 - acc: 0.9778 - val_loss: 0.0808 - val_acc: 0.9770\n",
      "Epoch 217/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0707 - acc: 0.9784 - val_loss: 0.0806 - val_acc: 0.9775\n",
      "Epoch 218/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0686 - acc: 0.9785 - val_loss: 0.0810 - val_acc: 0.9769\n",
      "Epoch 219/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0707 - acc: 0.9781 - val_loss: 0.0802 - val_acc: 0.9770\n",
      "Epoch 220/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0690 - acc: 0.9789 - val_loss: 0.0802 - val_acc: 0.9771\n",
      "Epoch 221/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0705 - acc: 0.9779 - val_loss: 0.0811 - val_acc: 0.9768\n",
      "Epoch 222/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0687 - acc: 0.9787 - val_loss: 0.0807 - val_acc: 0.9768\n",
      "Epoch 223/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0707 - acc: 0.9773 - val_loss: 0.0807 - val_acc: 0.9772\n",
      "Epoch 224/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0699 - acc: 0.9790 - val_loss: 0.0804 - val_acc: 0.9772\n",
      "Epoch 225/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0674 - acc: 0.9790 - val_loss: 0.0806 - val_acc: 0.9776\n",
      "Epoch 226/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0683 - acc: 0.9789 - val_loss: 0.0804 - val_acc: 0.9768\n",
      "Epoch 227/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0668 - acc: 0.9791 - val_loss: 0.0807 - val_acc: 0.9770\n",
      "Epoch 228/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0693 - acc: 0.9785 - val_loss: 0.0806 - val_acc: 0.9767\n",
      "Epoch 229/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0687 - acc: 0.9785 - val_loss: 0.0806 - val_acc: 0.9768\n",
      "Epoch 230/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0684 - acc: 0.9786 - val_loss: 0.0801 - val_acc: 0.9767\n",
      "Epoch 231/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0671 - acc: 0.9793 - val_loss: 0.0799 - val_acc: 0.9772\n",
      "Epoch 232/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0681 - acc: 0.9786 - val_loss: 0.0801 - val_acc: 0.9771\n",
      "Epoch 233/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0648 - acc: 0.9796 - val_loss: 0.0806 - val_acc: 0.9776\n",
      "Epoch 234/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0678 - acc: 0.9782 - val_loss: 0.0803 - val_acc: 0.9779\n",
      "Epoch 235/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0664 - acc: 0.9791 - val_loss: 0.0795 - val_acc: 0.9772\n",
      "Epoch 236/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0687 - acc: 0.9786 - val_loss: 0.0795 - val_acc: 0.9775\n",
      "Epoch 237/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0650 - acc: 0.9797 - val_loss: 0.0800 - val_acc: 0.9773\n",
      "Epoch 238/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0664 - acc: 0.9795 - val_loss: 0.0804 - val_acc: 0.9775\n",
      "Epoch 239/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0669 - acc: 0.9792 - val_loss: 0.0807 - val_acc: 0.9774\n",
      "Epoch 240/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0688 - acc: 0.9786 - val_loss: 0.0803 - val_acc: 0.9775\n",
      "Epoch 241/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0648 - acc: 0.9799 - val_loss: 0.0807 - val_acc: 0.9771\n",
      "Epoch 242/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0656 - acc: 0.9790 - val_loss: 0.0798 - val_acc: 0.9778\n",
      "Epoch 243/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0658 - acc: 0.9793 - val_loss: 0.0796 - val_acc: 0.9774\n",
      "Epoch 244/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0656 - acc: 0.9796 - val_loss: 0.0798 - val_acc: 0.9777\n",
      "Epoch 245/250\n",
      "48000/48000 [==============================] - 2s - loss: 0.0634 - acc: 0.9806 - val_loss: 0.0798 - val_acc: 0.9772\n",
      "Epoch 246/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0628 - acc: 0.9802 - val_loss: 0.0810 - val_acc: 0.9772\n",
      "Epoch 247/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0621 - acc: 0.9803 - val_loss: 0.0801 - val_acc: 0.9777\n",
      "Epoch 248/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0633 - acc: 0.9803 - val_loss: 0.0802 - val_acc: 0.9772\n",
      "Epoch 249/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0636 - acc: 0.9799 - val_loss: 0.0804 - val_acc: 0.9777\n",
      "Epoch 250/250\n",
      "48000/48000 [==============================] - 3s - loss: 0.0616 - acc: 0.9808 - val_loss: 0.0801 - val_acc: 0.9776\n",
      " 9728/10000 [============================>.] - ETA: 0sTest score: 0.0773787480356\n",
      "Test accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that training accuracy should still be above the test accuracy, otherwise we are not training long enough. So let's try to increase significantly the number of epochs up to 250, and we get 98.08% accuracy on training, 97.79% on validation, and 97.76% on the test:      \n",
    "请注意，训练准确率仍应高于测试准确率，否则我们不会训练足够长的时间。 所以我们试着大幅增加训练次数到250，训练的准确率达到98.08％，验证的准确率达97.76％，测试集的达97.79％：         \n",
    "![](http://i2.muimg.com/588926/8c5eb6f03b1bfd8b.png)     \n",
    "\n",
    "It is useful to observe how accuracy increases on training and test sets when the number of epochs increases. As you can see in the following graph, these two curves touch at about 250 epochs, and therefore, there is no need to train further after that point: \n",
    "当时训练次数增加时，观察训练和测试集的准确率是如何增加的，这点很有用。 如下图所示，这两条曲线约在训练250次时相交，因此，在此之后，无需进一步训练：\n",
    "![](http://i2.muimg.com/588926/66ced1e3eef91bd4.png) \n",
    "   \n",
    "Note that it has been frequently observed that networks with random dropout in internal hidden layers can generalize better on unseen examples contained in test sets. Intuitively, one can think of this as each neuron becoming more capable because it knows it cannot depend on its neighbors. During testing, there is no dropout, so we are now using all our highly tuned neurons. In short, it is generally a good approach to test how a net performs when some dropout function is adopted.         \n",
    "注意，我们经常观察到，内部隐藏层中具有随机dropout层的网络可以更好地对包含未知样本的测试集进行推理。因为它知道它不能依赖于相邻的神经元，所以每个神经元将变得更有效，这一点人们能够很直观地想到。 进行测试时，可以先不设置dropout层，我们运行所有经过高度调整过的神经元。 简而言之，测试一个网络执行某些dropout函数时的运行情况，这通常是一种很好的方法。    \n",
    "\n",
    "### Testing different optimizers in Keras\n",
    "### 在Keras中测试不同的优化器       \n",
    "We have defined and used a network; it is useful to start giving an intuition about how networks are trained. Let's focus on one popular training technique known as gradient descent (GD). Imagine a generic cost function $ C(w)$in one single variable w like in the following graph:\n",
    "我们定义和使用了一个网络; 开始给出关于网络如何训练的直觉是有用的。 我们专注于一种被称为梯度下降（GD）的流行训练技术。 想象一个含有单一变量w的一般损失函数$C（w）$，如下图所示：     \n",
    "![](http://i2.muimg.com/588926/dfc674d5766458c9.png)         \n",
    "The gradient descent can be seen as a hiker who aims at climbing down a mountain into a valley. The mountain represents the function $C$, while the valley represents the minimum $C_{min}$ . The hiker has a starting point $w_0$ . The hiker moves little by little. At each step$r$, the gradient is the direction of maximum increase.Mathematically, this direction is the value of the partial derivative $ \\frac{ac}{\\partial w} $  evaluated at point $w_r$ reached at step $r$. Therefore by taking the opposite direction,  $ -\\frac{ac}{\\partial w}(w_r) $ , the hiker can move towards the valley. At each step, the hiker can decide what the leg length is before the next step. This is the learning rate $\\eta\\geq0$ in gradient descent jargon. Note that if $\\eta$ is too small, then the hiker will move slowly. However, if $\\eta$ is too high, then the hiker will possibly miss the valley.         \n",
    "梯度下降可以被看作是一个从山坡攀登到山谷进行攀爬的人。 山代表函数$C$，而谷代表最小$C_{min}$。 徒步旅行者起点为$W_0$。 攀登者进行一点点移动。 对于每一步$r$，梯度是最大增加的方向。在数学上，该方向是在第$r$步达到的点$w_r$处估计的偏导数的值 $ \\frac{ac}{\\partial w} $ 。 因此，通过采取相反的方向，， $ -\\frac{ac}{\\partial w}(w_r) $，攀爬者可以向山谷移动。 对于每一步，徒步旅行者可以在下一步之前决定步长。 这是梯度下降术语中的学习率$\\eta\\geq0$。 请注意，如果$\\eta$太小，那么攀爬者会慢慢移动。 然而，如果$\\eta$太高，那么攀爬者可能会错过山谷。         \n",
    "\n",
    "Now you should remember that a sigmoid is a continuous function, and it is possible to compute the derivative. It can be proven that the sigmoid is shown as follows:        \n",
    "现在你应该记住，sigmoid 是一个连续的函数，可以计算导数。 可以证明，sigmoid如下所示：  \n",
    "$$\\sigma(x)= \\frac{1}{1+e^{-x}}$$    \n",
    "It has the following derivative:       \n",
    "它的导数如下：\n",
    "$$ \\frac{d \\sigma(x)}{d(x)}=\\sigma(x)(1-\\sigma(x))$$       \n",
    "ReLU is not differentiable in 0. We can, however, extend the first derivative in 0 to a function over the whole domain by choosing it to be either 0 or 1. The point-wise derivative $y=max(0,x)$ of ReLU  is as follows:       \n",
    "ReLU在0中是不可区分的。然而，我们可以通过选择0或1将整数域中的一阶导数扩展到整个域中。ReLU 函数$y=max(0,x)$的逐点导数如下： \n",
    "$$\\frac{dy}{dx}=\\left\\{\\begin{array}\\\\0\\quad x\\leq0\\\\1\\quad x>0\\end{array}\\right.$$         \n",
    "\n",
    "Once we have the derivative, it is possible to optimize the nets with a gradient descent technique.Keras uses its backend (either TensorFlow or Theano) for computing the derivative on our behalf so we don't need to worry about implementing or computing it. We just choose the activation function,and Keras computes its derivative on our behalf.        \n",
    "一旦我们有了导数，就可以用梯度下降技术来优化网络.Keras使用它的后端（TensorFlow或者Theano）来代替我们计算导数，所以我们不用担心实现或计算它。 我们只是选择激活函数，则可以用Keras代替我们计算其导数。     \n",
    "\n",
    "A neural network is essentially a composition of multiple functions with thousands, and sometimes millions, of parameters. Each network layer computes a function whose error should be minimized in order to improve the accuracy observed during the learning phase. When we discuss backpropagation, we will discover that the minimization game is a bit more complex than our toy example. However, it is still based on the same intuition of descending a valley.          \n",
    "神经网络本质上是具有数千个有时数百万个参数的多个功能的组合体。 每个网络层计算一个其误差应该最小化函数，以提高在学习阶段观察到的精度。 当我们讨论反向传播时，我们将发现最小化“算法游戏”比我们的玩具示例更复杂一些。 然而，它仍然源于一个求从山谷下降过程的直观算法。   \n",
    "\n",
    "Keras implements a fast variant of gradient descent known as stochastic gradient descent (SGD) and two more advanced optimization techniques known as RMSprop and Adam. RMSprop and Adam include the concept of momentum (a velocity component) in addition to the acceleration component that SGD has. This allows faster convergence at the cost of more computation. A full list of Keras-supported optimizers is at  https://keras.io/optimizers/ . SGD was our default choice so far. So now let's try the other two. It is very simple, we just need to change few lines:        \n",
    "Keras实现了称为随机梯度下降（SGD）的梯度下降的快速变体技术以及称为RMSprop和Adam的两种更先进的优化技术。 除了SGD的加速度分量之外，RMSprop和Adam还包括动量概念（速度分量）。 这样可以以更多的计算为代价实现更快的收敛。 Keras支持的优化器的完整列表如下链接https://keras.io/optimizers 。SGD是我们的默认选择。 所以现在让我们试试另外两个。 这很简单，我们只需要改几行：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "...\n",
    "OPTIMIZER = RMSprop() # optimizer,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's test it as shown in the following screenshot:           \n",
    "这样就行了。 我们来测试它，如下面的屏幕截图所示（这里直接修改代码得出结果）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282.0\n",
      "Trainable params: 118,282.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.4773 - acc: 0.8567 - val_loss: 0.1839 - val_acc: 0.9450\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2270 - acc: 0.9324 - val_loss: 0.1401 - val_acc: 0.9580\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1758 - acc: 0.9471 - val_loss: 0.1180 - val_acc: 0.9663\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.1514 - acc: 0.9549 - val_loss: 0.1151 - val_acc: 0.9669\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1343 - acc: 0.9605 - val_loss: 0.1038 - val_acc: 0.9708\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1236 - acc: 0.9630 - val_loss: 0.0953 - val_acc: 0.9718\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1139 - acc: 0.9657 - val_loss: 0.1009 - val_acc: 0.9717\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1062 - acc: 0.9689 - val_loss: 0.0994 - val_acc: 0.9732\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0987 - acc: 0.9705 - val_loss: 0.0967 - val_acc: 0.9748\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0950 - acc: 0.9713 - val_loss: 0.0943 - val_acc: 0.9744\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.0898 - acc: 0.9725 - val_loss: 0.0956 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0872 - acc: 0.9739 - val_loss: 0.0939 - val_acc: 0.9747\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0850 - acc: 0.9752 - val_loss: 0.0934 - val_acc: 0.9753\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0809 - acc: 0.9755 - val_loss: 0.0981 - val_acc: 0.9779\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0796 - acc: 0.9759 - val_loss: 0.1001 - val_acc: 0.9771\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.0767 - acc: 0.9776 - val_loss: 0.0983 - val_acc: 0.9762\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0758 - acc: 0.9779 - val_loss: 0.1072 - val_acc: 0.9762\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0767 - acc: 0.9778 - val_loss: 0.1027 - val_acc: 0.9765\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0713 - acc: 0.9788 - val_loss: 0.1091 - val_acc: 0.9766\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0718 - acc: 0.9789 - val_loss: 0.1071 - val_acc: 0.9763\n",
      " 9120/10000 [==========================>...] - ETA: 0sTest score: 0.0948472297508\n",
      "Test accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "OPTIMIZER = RMSprop() # optimizer,\n",
    "\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the preceding screenshot, RMSprop is faster than SDG since we are able to achieve an accuracy of 97.89% on training, 97.63% on validation, and 97.95% on the test improving SDG with only 20 iterations. For the sake of completeness, let's see how the accuracy and loss change with the number of epochs, as shown in the following graphs:          \n",
    "如上图所示，RMSprop比SDG快，因为我们能够在训练集上达到97.89％的准确率，验证时为97.63％，测试时为97.95％，只用20次训练就改进了SDG的结果。 为了完整起见，让我们看看准确性和损失如何随时代的变化而变化，如下图所示：        \n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGDCAYAAAD9K8D/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33f16b14a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘出分类准确率曲线\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try the other optimizer,  Adam() . It is pretty simple, as follows:              \n",
    "好的，让我们试试另一个优化器Adam（）。 这很简单，如下所示(此处修改代码直接执行)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282.0\n",
      "Trainable params: 118,282.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.5190 - acc: 0.8432 - val_loss: 0.1881 - val_acc: 0.9435\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2337 - acc: 0.9299 - val_loss: 0.1407 - val_acc: 0.9584\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1819 - acc: 0.9458 - val_loss: 0.1148 - val_acc: 0.9660\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.1535 - acc: 0.9541 - val_loss: 0.1077 - val_acc: 0.9671\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.1322 - acc: 0.9596 - val_loss: 0.1009 - val_acc: 0.9705\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.1188 - acc: 0.9632 - val_loss: 0.0894 - val_acc: 0.9729\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1059 - acc: 0.9674 - val_loss: 0.0899 - val_acc: 0.9722\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0992 - acc: 0.9699 - val_loss: 0.0880 - val_acc: 0.9753\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0899 - acc: 0.9725 - val_loss: 0.0834 - val_acc: 0.9750\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0863 - acc: 0.9730 - val_loss: 0.0811 - val_acc: 0.9760\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0820 - acc: 0.9734 - val_loss: 0.0854 - val_acc: 0.9756\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0765 - acc: 0.9755 - val_loss: 0.0785 - val_acc: 0.9773\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0723 - acc: 0.9759 - val_loss: 0.0776 - val_acc: 0.9765\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0687 - acc: 0.9779 - val_loss: 0.0816 - val_acc: 0.9771\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0693 - acc: 0.9776 - val_loss: 0.0786 - val_acc: 0.9785\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0627 - acc: 0.9798 - val_loss: 0.0807 - val_acc: 0.9773\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0592 - acc: 0.9800 - val_loss: 0.0804 - val_acc: 0.9778\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0577 - acc: 0.9810 - val_loss: 0.0793 - val_acc: 0.9785\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0569 - acc: 0.9820 - val_loss: 0.0856 - val_acc: 0.9776\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0557 - acc: 0.9818 - val_loss: 0.0803 - val_acc: 0.9772\n",
      "10000/10000 [==============================] - 0s     \n",
      "Test score: 0.0784350481694\n",
      "Test accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "#OPTIMIZER = RMSprop() # optimizer,\n",
    "OPTIMIZER = Adam() # optimizer\n",
    "\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, Adam is slightly better. With Adam, we achieve 98.18% accuracy on training,97.72% on validation, and 97.85% on the test with 20 iterations, as shown in the following graphs:         \n",
    "正如我们所看到的，Adam 稍好些。 采用Adam，我们的准确率准确率达到98.18％，验证的准确率达到97.72％，测试时达到97.85％，训练次数为20次，如下图所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGDCAYAAAD9K8D/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33f01b8eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘出分类准确率曲线\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our fifth variant, and remember that our initial baseline was at 92.27%.\n",
    "这是我们的第五个修改的程序，并记住我们的初始准确率是92.27％。         \n",
    "\n",
    "So far, we made progressive improvements; however, the gains are now more and more difficult.Note that we are optimizing with a dropout of 30%. For the sake of completeness, it could be useful to report the accuracy on the test only for other dropout values with  Adam() chosen as optimizer, as shown in the following graph:        \n",
    "到目前为止，我们逐步完善; 然而，收益现在越来越困难。请注意，我们使用的是随机剔除参数dropout为30％进行优化。 为了完整起见，我们做一个设置其他dropout参数值的Adam（）优化器的测试准确性报告，如下图所示：\n",
    "![](http://i4.buimg.com/588926/3f428119186f9a53.png)      \n",
    "### Controlling the optimizer learning rate \n",
    "### 控制优化学习率     \n",
    "There is another attempt we can make, which is changing the learning parameter for our optimizer. As you can see in the following graph, the optimal value is somewhere close to 0.001, which is the default learning rate for the optimer. Good! Adam works well out of the box:         \n",
    "我们还可以做另一个尝试，这就是改变我们的优化器的学习参数。 如下图所示，优化器的值接近0.001，这是优化器的默认学习率。 好！  Adam优化器工作得很好：     \n",
    "![](http://i2.muimg.com/588926/cd254eb24015439b.png) \n",
    "### Increasing the number of internal hidden neurons\n",
    "### 增加内部隐藏神经元的数量         \n",
    "We can make yet another attempt, that is, changing the number of internal hidden neurons. We report the results of the experiments with an increasing number of hidden neurons. We can see in the following graph that by increasing the complexity of the model, the run time increases significantly because there are more and more parameters to optimize. However, the gains that we are getting by increasing the size of the network decrease more and more as the network grows:           \n",
    "我们可以做另一个尝试，也就是改变内部隐藏神经元的数量。 我们用越来越多的隐藏神经元得出实验结果。 我们可以在下图中看到，通过增加模型的复杂性，运行时间显着增加，因为有越来越多的参数需要进行优化。 然而，随着网络的增长，我们通过增加网络规模获得的收益越来越多：      \n",
    "![](http://i4.buimg.com/588926/a5b78af98c1f7847.png)     \n",
    "In the following graph, we show the time needed for each iteration as the number of hidden neurons grow:    \n",
    "在下图中，我们显示每次迭代所需的时间，因为隐藏的神经元的数量增长：       \n",
    "![](http://i2.muimg.com/588926/342376066c2588ae.png) \n",
    "The following graph shows the accuracy as the number of hidden neurons grow:        \n",
    "下图显示了隐藏神经元数量增长的准确性：      \n",
    "![](http://i2.muimg.com/588926/d51bffe741d1b335.png)        \n",
    "### Increasing the size of batch computation       \n",
    "### 增加batch计算的大小     \n",
    "Gradient descent tries to minimize the cost function on all the examples provided in the training sets and, at the same time, for all the features provided in the input. Stochastic gradient descent is a much less expensive variant, which considers only  BATCH_SIZE examples. So, let's see what the behavior is by changing this parameter. As you can see, the optimal accuracy value is reached for  BATCH_SIZE=128 :       \n",
    "对于所有的输入特征变量，梯度下降尝试将训练集中提供的所有样本的损失函数最小化。 随机梯度下降是一个不那么昂贵的变体，它只考虑BATCH_SIZE样本。 所以，我们来看看改变这个参数将导致什么行为。 如你所见，BATCH_SIZE = 128时达到了获得了最好的准确率：      \n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-15/57765380-file_1494832767337_693c.png)     \n",
    "### Summarizing the experiments run for recognizing handwritten charts\n",
    "### 总结识别手写实验的图表\n",
    "So, let's summarize: with five different variants, we were able to improve our performance from 92.27% to 98.04%. First, we defined a simple layer network in Keras. Then, we improved the performance by adding some hidden layers. After that, we improved the performance on the test set by adding a few random dropouts to our network and by experimenting with different types of optimizers. Current results are summarized in the following table:      \n",
    "所以我们总结一下：有五种不同的修改过得程序，我们可以将我们的成绩从92.27％提高到98.04％。 首先，我们在Keras中定义了一个简单的层网络。 然后，我们通过添加一些隐藏层来提高成绩。 之后，我们通过在我们的网络中添加了一些随机的dropouts参数值并通过实验不同类型的优化器来提高测试集的成绩。 目前的结果总结如下表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model/Accuracy | Training      | Validation  |Test |\n",
    "| -------------  |:-------------:| -----------:|----:|\n",
    "| Simple         |    92.23%           |  92.41%           |  92.27%   |\n",
    "| Two hidden(128)|       94.55%        |    94.9%         |   94.62%  |\n",
    "| Dropout (30%)  |       98.08%        |       97.76%      |     97.79%|\n",
    "|RMSprop         |     97.89%          |       97.63%      |  97.95%   |\n",
    "| Adam           |        98.18%       |      97.72%       |    97.85% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the next two experiments did not provide significant improvements. Increasing the number of internal neurons creates more complex models and requires more expensive computations, but it provides only marginal gains. We get the same experience if we increase the number of training epochs. A final experiment consisted in changing the  BATCH_SIZE for our optimizer.       \n",
    "然而，接下来的两个实验没有提供显着的改进。 增加内部神经元的数量会产生更复杂的模型，需要更昂贵的计算，但它只能提供微小的收益。 如果我们增加训练次数的数量，我们将获得相同的经验。 最后一个实验中对于优化器更改了BATCH_SIZE的数量。       \n",
    "### Adopting regularization for avoiding overfitting\n",
    "### 采取正则化避免过拟\n",
    "Intuitively, a good machine learning model should achieve low error on training data. Mathematically,this is equivalent to minimizing the loss function on the training data given the machine learning model built. This is expressed by the following formula：\n",
    "直观地说，良好的机器学习模式应该在训练数据上实现低误差。 在数学上，这相当于在建立机器学习模型时最小化训练数据的损失函数。 这由以下公式表示：\n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-15/5692347-file_1494836313889_abbd.png)          \n",
    "However, this might not be enough. A model can become excessively complex in order to capture all the relations inherently expressed by the training data. This increase of complexity might have two negative consequences. First, a complex model might require a significant amount of time to be executed. Second, a complex model can achieve very good performance on training data—because all the inherent relations in trained data are memorized, but not so good performance on validation data—as the model is not able to generalize on fresh unseen data. Again, learning is more about generalization than memorization. The following graph represents a typical loss function decreasing on both validation and training sets. However, a certain point the loss on validation starts to increase because of overfitting:        \n",
    "但是，这可能还不够。 模型可能变得过于复杂，以便捕获训练数据固有表达的所有关系。 这种复杂性的增加可能会产生两个负面后果。 首先，复杂的模型可能需要大量的时间来执行。 第二，复杂的模型可以在训练数据方面取得非常好的表现，因为训练数据中的所有内在关系都被保留下来，但验证数据的性能并不好，因为模型不能推理出新的隐藏的数据。 再者，学习更多的是泛化而不是记忆。 以下图表示在验证和训练集上减少的典型损失函数。 然而，一定程度上，由于过度拟合，验证集上的损失度开始增加：    \n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-15/84989417-file_1494838844187_57bb.png)     \n",
    "As a rule of thumb, if during the training we see that the loss increases on validation, after an initial decrease, then we have a problem of model complexity that overfits training. Indeed, overfitting is the word used in machine learning for concisely describing this phenomenon.      \n",
    "根据经验，如果在训练期间，我们看到，验证的损失在首次下降之后增加，那么我们有一个过拟训练的模型复杂性问题。 事实上，过度拟合是机器学习中简单描述这种现象常见的词。        \n",
    "\n",
    "In order to solve the overfitting problem, we need a way to capture the complexity of a model, that is,how complex a model can be. What could be the solution? Well, a model is nothing more than a vector of weights. Therefore the complexity of a model can be conveniently represented as the number of nonzero weights. In other words, if we have two models, M1 and M2, achieving pretty much the same performance in terms of loss function, then we should choose the simplest model that has the minimum number of nonzero weights. We can use a hyperparameter >=0 for controlling what the importance of having a simple model is, as in this formula:        \n",
    "为了解决过度拟合问题，我们需要一种方法来捕捉模型的复杂性，即模型的复杂程度。 用什么可以解决这个问题？ 其实，一个模型只不过是一个权重的向量。 因此，模型的复杂性可以方便地表示为非零权重的数量。 换句话说，如果我们有两个模型M1和M2，在损失函数上实现了几乎相同的性能，那么我们应该选择具有最小数量的非零权重的最简单的模型。 我们可以使用超参数> = 0来控制具有简单模型的重要向量，如以下公式：     \n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-16/82723873-file_1494898778842_34b.png)        \n",
    "There are three different types of regularizations used in machine learning:\n",
    "机器学习有三种不同类型的正则化过程： \n",
    "* L1 regularization (also known as lasso): The complexity of the model is expressed as the sum of the absolute values of the weights\n",
    "* L1正则化（也称为lasso）：模型的复杂度表示为权重的绝对值之和\n",
    "\n",
    "* L2 regularization (also known as ridge): The complexity of the model is expressed as the sum of the squares of the weights\n",
    "* L2正则化（也称为ridge）：模型的复杂度表示为权重的平方和\n",
    "\n",
    "* Elastic net regularization: The complexity of the model is captured by a combination of the two preceding techniques\n",
    "* 弹性网络正则化：模型的复杂性通过前述两种技术的组合来获取  \n",
    "\n",
    "Note that the same idea of regularization can be applied independently to the weights, to the model,and to the activation. \n",
    "注意，正则化的相同想法可以独立地应用于权重，模型或者激活函数。      \n",
    "\n",
    "Therefore, playing with regularization can be a good way to increase the performance of a network,in particular when there is an evident situation of overfitting. This set of experiments is left as an exercise for the interested reader.        \n",
    "因此，进行正规化操作可以成为提高网络性能的好方法，特别是在出现过度拟合的情况时。 这些实验可以作为感兴趣的读者的练习。     \n",
    "\n",
    "Note that Keras supports both L1, L2, and elastic net regularizations. Adding regularization is easy; for instance, here we have a  L2 regularizer for kernel (the weight W):         \n",
    "请注意，Keras支持L1，L2和弹性网格正则化。 增加正则化的方法很容易; 例如，这里我们有一个内核的L2正则化（权重W）的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-82dccbd5595e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-82dccbd5595e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A full description of the available parameters is available at:  https://keras.io/regularizers/             \n",
    "有关可用参数的完整说明，请访问：https://keras.io/regularizers/         \n",
    "### Hyperparameters tuning     \n",
    "### 超参数调整          \n",
    "The preceding experiments gave a sense of what the opportunities for fine-tuning a net are. However,what is working for this example is not necessarily working for other examples. For a given net, there are indeed multiple parameters that can be optimized (such as the number of  hidden neurons ,  BATCH_SIZE ,number of  epochs , and many more according to the complexity of the net itself).         \n",
    "上述实验给出了一种微调网络的因素的方式。 然而，这个例子的工作不一定适用于其他例子。 对于给定的网络，确实可以根据网络本身的复杂性优化多个参数（如隐藏神经元的数量，BATCH_SIZE，训练次数等）。       \n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal combination of those parameters that minimize cost functions. The key idea is that if we have n parameters, then we can imagine that they define a space with n dimensions, and the goal is to find the point in this space which corresponds to an optimal value for the cost function. One way to achieve this goal is to create a grid in this space and systematically check for each grid vertex what the value assumed by the cost function is. In other words, the parameters are divided into buckets, and different combinations of values are checked via a brute force approach.         \n",
    "超参数调整是找到最小化成本函数的那些参数的最佳组合的过程。 关键思想是，如果我们有n个参数，那么我们可以想象它们了定义一个具有n维的空间，目的是找到这个空间中对应于成本函数的最优值的点。 实现此目标的一种方法是在此空间中创建一个网格，并系统地检查每个网格顶点是否具有成本函数假设的值。 换句话说，这些参数被划分为桶，并且通过暴力法来检查值的不同组合。   \n",
    "\n",
    "### Predicting output\n",
    "### 预测输出\n",
    "When a net is trained, it can be course be used for predictions. In Keras, this is very simple. We can use the following method:       \n",
    "当网络被训练时，它可以被用于预测。 在Keras中这很简单。 我们可以使用以下方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a0c7e72546e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given input, several types of output can be computed, including a method:\n",
    "对于给定的输入，可以计算出几种类型的输出，包括以下方法： \n",
    "* model.evaluate() : This is used to compute the loss values\n",
    "* model.evaluate（）：用于计算损失值\n",
    "\n",
    "* model.predict_classes() : This is used to compute category outputs\n",
    "* model.predict_classes（）：这用于计算类别输出\n",
    "\n",
    "* model.predict_proba() : This is used to compute class probabilities\n",
    "* model.predict_proba（）：这是用于计算类的概率      \n",
    "\n",
    "\n",
    "## A practical overview of backpropagation\n",
    "## 反向传播的实用概述\n",
    "Multilayer perceptrons learn from training data through a process called backpropagation. The process can be described as a way of progressively correcting mistakes as soon as they are detected.Let's see how this works.        \n",
    "多层感知器通过称为反向传播的过程从训练数据中学习。 该过程可以被描述为在检测到错误之后逐渐纠正错误的方法。看看它是如何工作的。        \n",
    "\n",
    "Remember that each neural network layer has an associated set of weights that determines the output values for a given set of inputs. In addition to that, remember that a neural network can have multiple hidden layers.       \n",
    "请记住，每个神经网络层都有一组相关的权重，用于确定给定输入集合的输出值。 除此之外，请记住神经网络可以有多个隐藏层。      \n",
    "\n",
    "In the beginning, all the weights have some random assignment. Then the net is activated for each input in the training set: values are propagated forward from the input stage through the hidden stages to the output stage where a prediction is made (note that we have kept the following diagram simple by only representing a few values with green dotted lines, but in reality, all the values are propagated forward through the network):      \n",
    "一开始，所有的权重都是被随机分配的。 然后，网络被训练集中的每个输入激活：值从输入阶段到隐藏阶段向前传播一直到进行预测的输出阶段（请注意，我们通过仅用绿色虚线表示几个值，让以下图表得以简单呈现，在现实中，所有的值都通过网络向前传播）：\n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-16/29763060-file_1494919741916_14cfc.png)    \n",
    "Since we know the true observed value in the training set, it is possible to calculate the error made in prediction. The key intuition for backtracking is to propagate the error back and use an appropriate optimizer algorithm, such as a gradient descent, to adjust the neural network weights with the goal of reducing the error (again for the sake of simplicity, only a few error values are represented):          \n",
    "由于我们知道训练集中的真实观察值，因此可以计算预测中的误差。 回溯的关键点是将错误传播回来，并使用适当的优化器算法（如梯度下降）来调整神经网络权重，目的是减少错误（为了简单起见，我们只需要几个错误值）：       \n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-16/36650866-file_1494920011324_112f1.png)\n",
    "The process of forward propagation from input to output and backward propagation of errors is repeated several times until the error gets below a predefined threshold. The whole process is represented in the following diagram:      \n",
    "从输入到输出的正向传播和错误的反向传播的过程需要重复几次，直到误差低于预定阈值。 整个过程如下图所示：     \n",
    "![](http://opzgwpokn.bkt.clouddn.com/17-5-16/15844639-file_1494920147987_9534.png)\n",
    "The features represent the input and the labels are here used to drive the learning process. The model is updated in such a way that the loss function is progressively minimized. In a neural network, what really matters is not the output of a single neuron but the collective weights adjusted in each layer.Therefore, the network progressively adjusts its internal weights in such a way that the prediction increases the number of labels correctly forecasted. Of course, using the right set features and having a quality labeled data is fundamental to minimizing the bias during the learning process.        \n",
    "功能表示输入，标签在这里用于驱动学习过程。 该模型以损失函数逐步最小化的方式进行更新。 在神经网络中，真正重要的不是单个神经元的输出，而是在每层中调整集体权重。因此，网络逐渐调整其内部权重，使得预测增加了正确预测的标签数量。 当然，使用正确的集合特征并具有高质量的标记数据是在学习过程中最小化偏差的基础。      \n",
    "\n",
    "\n",
    "## Towards a deep learning approach\n",
    "## 迈向深入学习的方法      \n",
    "While playing with handwritten digit recognition, we came to the conclusion that the closer we get to the accuracy of 99%, the more difficult it is to improve. If we want to have more improvements, we definitely need a new idea. What are we missing? Think about it.        \n",
    "在使用手写数字识别的同时，我们得出结论，我们越接近99％的准确度，改进越困难。 如果我们想要有更多的改进，我们肯定需要一个新的想法。 我们缺少什么？ 想想吧\n",
    "\n",
    "The fundamental intuition is that, so far, we lost all the information related to the local spatiality of the images. In particular, this piece of code transforms the bitmap, representing each written digit into a flat vector where the spatial locality is gone:        \n",
    "根本的点是，到目前为止，我们失去了与图像的局部空间相关的所有信息。 特别地，这段代码将位图转换成空间位置消失的平面向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not how our brain works. Remember that our vision is based on multiple cortex levels, each one recognizing more and more structured information, still preserving the locality. First we see single pixels, then from that, we recognize simple geometric forms and then more and more sophisticated elements such as objects, faces, human bodies, animals and so on.\n",
    "然而，这不是我们的大脑工作的方式。 请记住，我们的愿景是基于多个皮质层次，每个层级都认识到越来越多的结构化信息，仍然保留了本地化。 首先我们看到单像素，然后从中我们识别简单的几何形状，然后识别出越来越复杂的元素，如物体，人脸，人体，动物等等。        \n",
    "\n",
    "In  Chapter 3 , Deep Learning with ConvNets, we will see that a particular type of deep learning network known as convolutional neural network (CNN) has been developed by taking into account both the idea of preserving the spatial locality in images (and, more generally, in any type of information) and the idea of learning via progressive levels of abstraction: with one layer, you can only learn simple patterns; with more than one layer, you can learn multiple patterns. Before discussing CNN, we need to discuss some aspects of Keras architecture and have a practical introduction to a few additional machine learning concepts. This will be the topic of the next chapters.         \n",
    "在第3章，深度学习的ConvNets方法中，我们将看到，已经开发了一种特定类型的深度学习网络，称为卷积神经网络（CNN），CNN会考虑保存图像（甚至更一般地，任何类型的信息）中空间局部性的概念， 同时，我们看到有了通过渐进层次抽象学习的思想：若只有一层，你只能学习简单的模式; 具有多层，您则可以学习各种图案。 在讨论CNN之前，我们需要讨论Keras架构的一些方面，并在实践中引入几个额外的机器学习概念。 这将是下一章的主题。    \n",
    "## Summary\n",
    "## 概要           \n",
    "In this chapter, you learned the basics of neural networks, more specifically, what a perceptron is,what a multilayer perceptron is, how to define neural networks in Keras, how to progressively improve metrics once a good baseline is established, and how to fine-tune the hyperparameter's space. In addition to that, you now also have an intuitive idea of what some useful activation functions (sigmoid and ReLU) are, and how to train a network with backpropagation algorithms based on either gradient descent, on stochastic gradient descent, or on more sophisticated approaches, such as Adam and RMSprop.     \n",
    "在本章中，您学习了神经网络的基础知识，更具体地说，感知器是什么，多层感知器是什么，如何在Keras中定义神经网络，如果建立良好的基线，如何逐步改进指标，以及如何处理 调整超参数的空间。 此外，您现在还可以直观了解一些有用的激活功能（Sigmoid和ReLU），以及如何使用基于梯度下降，随机梯度下降或更复杂的方法的反向传播算法来训练网络 ，如Adam和RMSprop。        \n",
    "\n",
    "In the next chapter, we will see how to install Keras on AWS, Microsoft Azure, Google Cloud, and on your own machine. In addition to that, we will provide an overview of Keras APIs.\n",
    "在下一章中，我们将看到如何在AWS，Microsoft Azure，Google Cloud以及自己的机器上安装Keras。 除此之外，我们将提供Keras API的概述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
